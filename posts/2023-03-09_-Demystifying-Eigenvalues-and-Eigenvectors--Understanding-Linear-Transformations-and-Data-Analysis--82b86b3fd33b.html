<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>“Demystifying Eigenvalues and Eigenvectors: Understanding Linear Transformations and Data Analysis”</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">“Demystifying Eigenvalues and Eigenvectors: Understanding Linear Transformations and Data Analysis”</h1>
</header>
<section data-field="subtitle" class="p-summary">
Eigenvalues and eigenvectors are concepts in linear algebra used to understand and analyze the behavior of matrices. They have various…
</section>
<section data-field="body" class="e-content">
<section name="3dab" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="3017" id="3017" class="graf graf--h3 graf--startsWithDoubleQuote graf--leading graf--title">“Demystifying Eigenvalues and Eigenvectors: Understanding Linear Transformations and Data Analysis”</h3><figure name="0e0d" id="0e0d" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*0C-MXaKstn3dfJI_" data-width="6000" data-height="4000" data-unsplash-photo-id="5IHz5WhosQE" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*0C-MXaKstn3dfJI_"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@chrislawton?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@chrislawton?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener noopener" target="_blank">Chris Lawton</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener noopener" target="_blank">Unsplash</a></figcaption></figure><p name="347f" id="347f" class="graf graf--p graf-after--figure">Eigenvalues and eigenvectors are concepts in linear algebra used to understand and analyze the behavior of matrices. They have various applications, from computer graphics and physics to engineering and finance.</p><p name="1876" id="1876" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">To understand what eigenvalues and eigenvectors are, let’s start with some basic definitions:</strong></p><p name="5617" id="5617" class="graf graf--p graf-after--p">A matrix is a rectangular array of numbers representing a linear transformation. In other words, if you have a matrix A and a vector x, the product Ax will give you a new vector representing the result of applying a linear transformation to x.</p><p name="55ba" id="55ba" class="graf graf--p graf-after--p">An eigenvector of a matrix A is a non-zero vector x that, when multiplied by A, gives a scalar multiple of itself, i.e., Ax = λx, where λ is a scalar called the eigenvalue of A corresponding to x.</p><p name="7ba1" id="7ba1" class="graf graf--p graf-after--p">For example, we have a 2x2 matrix A = [3 1; 2 4]. To find its eigenvectors and eigenvalues, we must solve the equation Ax = λx for x and λ. This gives us the following system of equations:</p><blockquote name="382d" id="382d" class="graf graf--blockquote graf-after--p">(3 λ + 1) x1 = 0 (2 λ — 2) x2 = 0</blockquote><p name="0c9e" id="0c9e" class="graf graf--p graf-after--blockquote">We can solve for λ by setting the determinant of the matrix (A — λI) equal to zero, where I is the identity matrix. This gives us the equation λ² — 7λ + 10 = 0, which has solutions λ = 2 and λ = 5.</p><p name="0d7c" id="0d7c" class="graf graf--p graf-after--p">To find the eigenvectors corresponding to each eigenvalue, we substitute the values of λ into the system of equations and solve for x. For λ = 2, we get x = [-1; 1]. For λ = 5, we get x = [-1; 2].</p><p name="df84" id="df84" class="graf graf--p graf-after--p">So, the eigenvalues of A are λ1 = 2 and λ2 = 5, and the corresponding eigenvectors are x1 = [-1; 1] and x2 = [-1; 2].</p><p name="d677" id="d677" class="graf graf--p graf-after--p">Now, you might be wondering why eigenvectors and eigenvalues are important. The answer lies in the fact that they tell us how a matrix transforms vectors in certain directions. In particular, the eigenvectors tell us the directions in which the transformation stretches or compresses the vector. In contrast, the eigenvalues tell us the amount of stretching or compression in those directions.</p><p name="1f4b" id="1f4b" class="graf graf--p graf-after--p">For example, in our 2x2 matrix A = [3 1; 2 4], the eigenvector x1 = [-1; 1] corresponds to the eigenvalue λ1 = 2. This means that when we apply the transformation represented by A to any vector in the direction of x1, the vector will be scaled by a factor of 2. Similarly, the eigenvector x2 = [-1; 2] corresponds to the eigenvalue λ2 = 5, which means that vectors in the direction of x2 will be scaled by a factor of 5.</p><p name="f857" id="f857" class="graf graf--p graf-after--p">Eigenvalues and eigenvectors also have a number of important properties and applications in areas like data analysis, machine learning, and optimization. For example, in principal component analysis (PCA), eigenvectors are used to find the directions in which data points vary the most. In contrast, they can be used in linear regression to find the coefficients that minimize the difference between the predicted and actual values.</p><p name="e8f5" id="e8f5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Pseudocode:</strong></p><ol class="postList"><li name="4fae" id="4fae" class="graf graf--li graf-after--p">Check that matrix A is square (same number of rows and columns). If it is not square, stop and report an error.</li><li name="e224" id="e224" class="graf graf--li graf-after--li">Solve the characteristic equation det(A — λI) = 0 for the eigenvalues λ.</li><li name="d374" id="d374" class="graf graf--li graf-after--li">For each eigenvalue λ_i, solve the equation (A — λ_iI)x_i = 0 for the corresponding eigenvector x_i. This can be done by finding the null space of the matrix (A — λ_iI).</li><li name="add5" id="add5" class="graf graf--li graf-after--li">Normalize each eigenvector x_i by dividing it by its length (i.e., by dividing it by the square root of the sum of its squared components).</li><li name="8b9b" id="8b9b" class="graf graf--li graf-after--li">The eigenvalues λ and eigenvectors x form a set of pairs (λ_i, x_i) that satisfy the equation Ax = λx.</li></ol><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="php" name="7616" id="7616" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content">import numpy <span class="hljs-keyword">as</span> np<br /><br /><span class="hljs-comment"># Define the matrix</span><br />A = np.<span class="hljs-keyword">array</span>([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br /><br /><span class="hljs-comment"># Find eigenvalues and eigenvectors of A using numpy.linalg.eig</span><br />lambdas, eigenvectors = np.linalg.<span class="hljs-title function_ invoke__">eig</span>(A)<br /><br /><span class="hljs-comment"># Print the results</span><br /><span class="hljs-keyword">print</span>(<span class="hljs-string">&quot;Eigenvalues:&quot;</span>, lambdas)<br /><span class="hljs-keyword">print</span>(<span class="hljs-string">&quot;Eigenvectors:\n&quot;</span>, eigenvectors)</span></pre><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="lua" name="956d" id="956d" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">Eigenvalues: [<span class="hljs-number">-0.37228132</span>  <span class="hljs-number">5.37228132</span>]<br />Eigenvectors:<br /> <span class="hljs-string">[[-0.82456484 -0.41597356]<br /> [ 0.56576746 -0.90937671]]</span></span></pre><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="cbf0" id="cbf0" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><br /><span class="hljs-comment"># Define matrix A</span><br />A = np.array([[<span class="hljs-number">3</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">4</span>]])<br /><br /><span class="hljs-comment"># Calculate eigenvectors and eigenvalues</span><br />eigenvalues, eigenvectors = np.linalg.eig(A)<br /><br /><span class="hljs-comment"># Plot eigenvectors as arrows</span><br /><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(eigenvalues)):<br />    plt.arrow(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, eigenvectors[<span class="hljs-number">0</span>,i], eigenvectors[<span class="hljs-number">1</span>,i], head_width=<span class="hljs-number">0.1</span>, head_length=<span class="hljs-number">0.1</span>, fc=<span class="hljs-string">&#x27;k&#x27;</span>, ec=<span class="hljs-string">&#x27;k&#x27;</span>)<br />    plt.text(eigenvectors[<span class="hljs-number">0</span>,i]*<span class="hljs-number">1.1</span>, eigenvectors[<span class="hljs-number">1</span>,i]*<span class="hljs-number">1.1</span>, <span class="hljs-string">&#x27;Eigenvector {}&#x27;</span>.<span class="hljs-built_in">format</span>(i+<span class="hljs-number">1</span>))<br /><br /><span class="hljs-comment"># Plot original matrix as a line</span><br />x = np.array([[-<span class="hljs-number">2</span>, <span class="hljs-number">2</span>]])<br />y = x*A[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]/A[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br />plt.plot(x[<span class="hljs-number">0</span>], y[<span class="hljs-number">0</span>], <span class="hljs-string">&#x27;b-&#x27;</span>, label=<span class="hljs-string">&#x27;Original Matrix&#x27;</span>)<br /><br /><span class="hljs-comment"># Plot eigenvector-scaled matrix as a line</span><br /><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(eigenvalues)):<br />    x = np.array([[-<span class="hljs-number">2</span>*eigenvectors[<span class="hljs-number">0</span>,i], <span class="hljs-number">2</span>*eigenvectors[<span class="hljs-number">0</span>,i]]])<br />    y = x*A[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]/A[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]*eigenvectors[<span class="hljs-number">1</span>,i]/eigenvectors[<span class="hljs-number">0</span>,i]<br />    plt.plot(x[<span class="hljs-number">0</span>], y[<span class="hljs-number">0</span>], <span class="hljs-string">&#x27;r-&#x27;</span>, label=<span class="hljs-string">&#x27;Eigenvector-scaled Matrix {}&#x27;</span>.<span class="hljs-built_in">format</span>(i+<span class="hljs-number">1</span>))<br /><br /><span class="hljs-comment"># Add annotations for eigenvalues and eigenvectors</span><br /><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(eigenvalues)):<br />    plt.annotate(<span class="hljs-string">&#x27;Eigenvalue {:.2f}&#x27;</span>.<span class="hljs-built_in">format</span>(eigenvalues[i]), xy=(eigenvectors[<span class="hljs-number">0</span>,i], eigenvectors[<span class="hljs-number">1</span>,i]), xytext=(eigenvectors[<span class="hljs-number">0</span>,i]+<span class="hljs-number">0.5</span>, eigenvectors[<span class="hljs-number">1</span>,i]+<span class="hljs-number">0.5</span>), arrowprops=<span class="hljs-built_in">dict</span>(arrowstyle=<span class="hljs-string">&#x27;-&gt;&#x27;</span>))<br /><br />plt.xlim(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br />plt.ylim(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br />plt.legend(loc=<span class="hljs-string">&#x27;upper left&#x27;</span>)<br />plt.title(<span class="hljs-string">&#x27;Eigenvalues and Eigenvectors of Matrix A&#x27;</span>)<br />plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br />plt.grid()<br />plt.show()</span></pre><p name="8c99" id="8c99" class="graf graf--p graf-after--pre">This code will plot the eigenvectors of matrix A as arrows and show how the matrix transforms vectors in the directions of those eigenvectors. The blue line represents the original matrix, and the red lines show how the matrix transforms vectors in the directions of the eigenvectors, scaled by the corresponding eigenvalue.</p><figure name="4e7d" id="4e7d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1bbqmM3pYLezUjGngD4glA.png" data-width="570" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*1bbqmM3pYLezUjGngD4glA.png"></figure><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="makefile" name="ee19" id="ee19" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content">import numpy as np<br />import matplotlib.pyplot as plt<br /><br /><br />A = np.array([[2, 1], [1, 2]])<br /><br />eigenvalues = np.linalg.eigvals(A)<br /><br /><br /><span class="hljs-comment"># Vary the value of A[0,0] and plot the corresponding eigenvalues</span><br />x_values = np.linspace(0, 5, 100)<br />eigenvalues = []<br /><br />for x in x_values:<br />    A[0, 0] = x<br />    eigenvalues.append(np.linalg.eigvals(A))<br /><br />eigenvalues = np.array(eigenvalues)<br /><br />plt.plot(x_values, eigenvalues[:, 0], label=<span class="hljs-string">&quot;Eigenvalue 1&quot;</span>)<br />plt.plot(x_values, eigenvalues[:, 1], label=<span class="hljs-string">&quot;Eigenvalue 2&quot;</span>)<br />plt.xlabel(<span class="hljs-string">&quot;A[0,0]&quot;</span>)<br />plt.ylabel(<span class="hljs-string">&quot;Eigenvalues&quot;</span>)<br />plt.title(<span class="hljs-string">&quot;Eigenvalues of A as a function of A[0,0]&quot;</span>)<br />plt.legend()<br />plt.show()</span></pre><figure name="d70c" id="d70c" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*bnwRdTMm1LIGTrY47zAkXQ.png" data-width="554" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*bnwRdTMm1LIGTrY47zAkXQ.png"></figure><p name="6baa" id="6baa" class="graf graf--p graf-after--figure">As you can see, changing the value of A[0,0] affects the eigenvalues of A, and the line plot allows us to visualize this relationship.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="3a71" id="3a71" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><br />A = np.array([[<span class="hljs-number">2</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]])<br />initial_conditions = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>]])<br /><br />eigenvalues, eigenvectors = np.linalg.eig(A)<br /><br /><span class="hljs-comment"># Define a function to calculate the transformed vectors for each initial condition</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">transform_vectors</span>(<span class="hljs-params">initial_conditions, A</span>):<br />    transformed_vectors = []<br />    <span class="hljs-keyword">for</span> x0, y0 <span class="hljs-keyword">in</span> initial_conditions:<br />        v = np.array([x0, y0])<br />        transformed_vectors.append(A.dot(v))<br />    <span class="hljs-keyword">return</span> np.array(transformed_vectors)<br /><br /><span class="hljs-comment"># Calculate the transformed vectors for each initial condition</span><br />transformed_vectors = transform_vectors(initial_conditions, A)<br /><br /><span class="hljs-comment"># Plot the heatmap</span><br />plt.imshow(transformed_vectors, cmap=<span class="hljs-string">&#x27;coolwarm&#x27;</span>, aspect=<span class="hljs-string">&#x27;auto&#x27;</span>)<br />plt.colorbar()<br />plt.xlabel(<span class="hljs-string">&quot;Eigenvectors&quot;</span>)<br />plt.ylabel(<span class="hljs-string">&quot;Initial Conditions&quot;</span>)<br />plt.title(<span class="hljs-string">&quot;Eigenvalues and Eigenvectors of A&quot;</span>)<br />plt.xticks(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(eigenvectors)), [<span class="hljs-string">&#x27;v1&#x27;</span>, <span class="hljs-string">&#x27;v2&#x27;</span>])<br />plt.yticks(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(initial_conditions)), [<span class="hljs-string">f&quot;(<span class="hljs-subst">{x0}</span>, <span class="hljs-subst">{y0}</span>)&quot;</span> <span class="hljs-keyword">for</span> x0, y0 <span class="hljs-keyword">in</span> initial_conditions])<br />plt.show()</span></pre><figure name="082e" id="082e" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*zJWpla5UITRpiTOhxyLTuQ.png" data-width="575" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*zJWpla5UITRpiTOhxyLTuQ.png"></figure><p name="5ebd" id="5ebd" class="graf graf--p graf-after--figure">As you can see, the heatmap allows us to visualize how the transformed vectors change with respect to the initial conditions and the eigenvectors of the matrix. The color of each cell in the heatmap represents the magnitude and direction of the transformed vector, with blue indicating negative values and red indicating positive values. This can help to illustrate the concept of stability in a system and the importance of eigenvectors and eigenvalues in understanding system behavior.</p><figure name="7113" id="7113" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3r8hM2UQr2KTFZ0VduVzOg.png" data-width="1794" data-height="1714" src="https://cdn-images-1.medium.com/max/800/1*3r8hM2UQr2KTFZ0VduVzOg.png"></figure><p name="d9eb" id="d9eb" class="graf graf--p graf-after--figure">This code creates a grid of 25 subplots, each showing the eigenvectors and eigenvalues of matrix A at a different value of t. The eigenvectors are plotted as arrows, and the eigenvalue magnitudes are plotted as circles. As t changes, the eigenvectors and eigenvalues change, reflecting the matrix’s changing properties. You can modify the code to customize the plot settings, change the range of t values, or use a different matrix function A(t) to explore other examples.</p><p name="8eea" id="8eea" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Conclusion:</strong></p><p name="a5cf" id="a5cf" class="graf graf--p graf-after--p">Eigenvalues and eigenvectors are important mathematical concepts that play a significant role in many areas of science and engineering. Eigenvectors represent directions in a system that remains unchanged under linear transformations, while eigenvalues represent the scaling factor of the transformation along these directions. Understanding the properties of eigenvectors and eigenvalues can provide insights into the behavior and stability of systems.</p><p name="fb97" id="fb97" class="graf graf--p graf-after--p">In this blog post, we have discussed the basic concepts of eigenvalues and eigenvectors and how they can be calculated for a given matrix. We have also explored some common applications of eigenvalues and eigenvectors, such as principal component analysis, image compression, and differential equations.</p><p name="51b1" id="51b1" class="graf graf--p graf-after--p graf--trailing">To visualize the concepts of eigenvalues and eigenvectors, we have demonstrated several types of plots using Python, including line plots and heatmaps. These plots illustrate the relationship between eigenvalues, eigenvectors, and the underlying matrix and how they can be used to analyze and understand various systems.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@the_daft_introvert" class="p-author h-card">Vishal Sharma</a> on <a href="https://medium.com/p/82b86b3fd33b"><time class="dt-published" datetime="2023-03-09T20:51:15.317Z">March 9, 2023</time></a>.</p><p><a href="https://medium.com/@the_daft_introvert/demystifying-eigenvalues-and-eigenvectors-understanding-linear-transformations-and-data-analysis-82b86b3fd33b" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 2, 2023.</p></footer></article></body></html>