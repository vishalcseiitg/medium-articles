<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>“Simple yet Powerful: Comparing Least Squares and Nearest Neighbors for Prediction in Machine…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">“Simple yet Powerful: Comparing Least Squares and Nearest Neighbors for Prediction in Machine…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Introduction:
</section>
<section data-field="body" class="e-content">
<section name="d5e7" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="a642" id="a642" class="graf graf--h3 graf--startsWithDoubleQuote graf--leading graf--title">“Simple yet Powerful: Comparing Least Squares and Nearest Neighbors for Prediction in Machine Learning”</h3><figure name="9b93" id="9b93" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*bjaTrcNXUlezyo5Q" data-width="4592" data-height="2583" data-unsplash-photo-id="XoBdj1zV-EA" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*bjaTrcNXUlezyo5Q"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@m_malkovich?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@m_malkovich?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">petr sidorov</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure><h4 name="2009" id="2009" class="graf graf--h4 graf-after--figure">Introduction:</h4><p name="152c" id="152c" class="graf graf--p graf-after--h4">Prediction in machine learning refers to the process of using past data to make informed guesses about future outcomes. Machine learning models are trained on historical data to learn patterns and relationships, and then these models can be used to predict future outcomes based on new data.</p><p name="3bd6" id="3bd6" class="graf graf--p graf-after--p">There are many different methods for prediction in machine learning, ranging from simple linear regression to complex deep learning models. This article will focus on two simple approaches: least squares and nearest neighbors.</p><p name="000d" id="000d" class="graf graf--p graf-after--p">Least squares is a method for fitting a line or curve to a set of data points by minimizing the sum of the squared differences between the predicted values and the actual values. This approach is commonly used for linear regression and can also be used for polynomial regression. Least squares have the advantage of being simple and interpretable, but they can be sensitive to outliers. It may make assumptions about the linearity of the data that do not hold true in reality.</p><p name="0061" id="0061" class="graf graf--p graf-after--p">Nearest neighbors, on the other hand, is a non-parametric approach that does not make any assumptions about the underlying distribution of the data. This approach works by finding the k nearest data points to a new data point and using their values to make a prediction. Nearest neighbors is flexible and can work well in situations where the data is not linear or where there are complex interactions between variables. Still, it can also be sensitive to the choice of distance metric and may overfit when the number of features is large.</p><p name="5aa7" id="5aa7" class="graf graf--p graf-after--p">This article aims to explain these two approaches and compare their strengths and weaknesses. By understanding the advantages and limitations of each approach, readers will be better equipped to choose the appropriate method for their specific problem and data set.</p><h4 name="cd5c" id="cd5c" class="graf graf--h4 graf-after--p">Least Squares:</h4><p name="beae" id="beae" class="graf graf--p graf-after--h4">Least squares is a method for finding the line or curve that best fits a set of data points. Specifically, it seeks to minimize the sum of the squared differences between the predicted values and the actual values. The idea is to find the line or curve passing as close to all data points as possible.</p><p name="5cad" id="5cad" class="graf graf--p graf-after--p">Given a set of data points (x₁, y₁), (x₂, y₂), …, (xₙ, yₙ), we want to find the line or curve that best fits the data. Least squares do this by minimizing the sum of the squared differences between the predicted values and the actual values.</p><p name="c742" id="c742" class="graf graf--p graf-after--p">For a linear regression model with slope m and intercept b, the least squares solution is given by:</p><blockquote name="e10b" id="e10b" class="graf graf--blockquote graf-after--p">m = ∑(xᵢ — x̄)(yᵢ — ȳ) / ∑(xᵢ — x̄)²</blockquote><blockquote name="89cb" id="89cb" class="graf graf--blockquote graf-after--blockquote">b = ȳ — m x̄</blockquote><blockquote name="28aa" id="28aa" class="graf graf--blockquote graf-after--blockquote">where x̄ and ȳ are the mean values of x and y, respectively.</blockquote><p name="f0ed" id="f0ed" class="graf graf--p graf-after--blockquote">For polynomial regression, the least squares solution involves finding the coefficients of the polynomial that minimize the sum of the squared errors.</p><p name="77d1" id="77d1" class="graf graf--p graf-after--p">In practice, least squares are often used in regression analysis to model the relationship between two or more variables. For example, we might use linear regression to model the relationship between a person’s height and weight. We would collect data points, including a person’s height and weight, and then use least squares to find the line that best fits the data.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="22bb" id="22bb" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><br /><span class="hljs-comment"># Generate some random data</span><br />np.random.seed(<span class="hljs-number">0</span>)<br />x = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>)<br />y = <span class="hljs-number">2</span> * x + <span class="hljs-number">1</span> + np.random.normal(size=<span class="hljs-number">100</span>)<br /><br /><span class="hljs-comment"># Fit a line to the data using least squares</span><br />A = np.vstack([x, np.ones_like(x)]).T<br />m, b = np.linalg.lstsq(A, y, rcond=<span class="hljs-literal">None</span>)[<span class="hljs-number">0</span>]<br /><br /><span class="hljs-comment"># Plot the data and the fitted line</span><br />plt.scatter(x, y, s=<span class="hljs-number">10</span>, c=<span class="hljs-string">&#x27;b&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br />plt.plot(x, m*x + b, <span class="hljs-string">&#x27;r&#x27;</span>, linewidth=<span class="hljs-number">2</span>)<br />plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br />plt.title(<span class="hljs-string">&#x27;Linear Regression with Least Squares&#x27;</span>)<br />plt.show()</span></pre><p name="bf4d" id="bf4d" class="graf graf--p graf-after--pre">In this example, we generate random data and add random noise to simulate measurement error. We then use <code class="markup--code markup--p-code">numpy.linalg.lstsq()</code> to find the values of <code class="markup--code markup--p-code">m</code> and <code class="markup--code markup--p-code">b</code> that minimizes the sum of the squared errors between the predicted values (<code class="markup--code markup--p-code">mx + b</code>) and the actual values (<code class="markup--code markup--p-code">y</code>). Finally, we plot the data and the fitted line.</p><figure name="579d" id="579d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ZqmjbED3DjzGLxvfjVAV1A.png" data-width="563" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*ZqmjbED3DjzGLxvfjVAV1A.png"></figure><p name="fc9a" id="fc9a" class="graf graf--p graf-after--figure">However, there are some limitations to least squares. One is that it can be sensitive to outliers, meaning that a few extreme data points can significantly affect the fitted line or curve. Another limitation is that least squares assume that the relationship between the variables is linear, which may only be true in some cases.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="6707" id="6707" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><br /><span class="hljs-comment"># Generate some random data</span><br />np.random.seed(<span class="hljs-number">0</span>)<br />x = np.linspace(-<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">100</span>)<br />y = <span class="hljs-number">0.5</span> * x**<span class="hljs-number">3</span> + <span class="hljs-number">2</span> * x**<span class="hljs-number">2</span> - <span class="hljs-number">3</span> * x - <span class="hljs-number">1</span> + np.random.normal(size=<span class="hljs-number">100</span>)<br /><br /><span class="hljs-comment"># Fit a polynomial to the data using least squares</span><br />X = np.vstack([x**<span class="hljs-number">3</span>, x**<span class="hljs-number">2</span>, x, np.ones_like(x)]).T<br />p = np.linalg.lstsq(X, y, rcond=<span class="hljs-literal">None</span>)[<span class="hljs-number">0</span>]<br /><br /><span class="hljs-comment"># Evaluate the polynomial at some new points</span><br />x_new = np.linspace(-<span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">200</span>)<br />y_new = np.polyval(p, x_new)<br /><br /><span class="hljs-comment"># Plot the data and the fitted polynomial</span><br />plt.scatter(x, y, s=<span class="hljs-number">15</span>, c=<span class="hljs-string">&#x27;b&#x27;</span>, alpha=<span class="hljs-number">0.6</span>)<br />plt.plot(x_new, y_new, <span class="hljs-string">&#x27;r&#x27;</span>)<br />plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br />plt.title(<span class="hljs-string">&#x27;Linear Regression with Least Squares for a Cubic Polynomial&#x27;</span>)<br /><br />plt.show()</span></pre><p name="99ba" id="99ba" class="graf graf--p graf-after--pre">In this example, we generate random data following a cubic polynomial. We then use <code class="markup--code markup--p-code">numpy.linalg.lstsq()</code> to find the values of the coefficients that minimize the sum of the squared errors between the predicted values (<code class="markup--code markup--p-code">p[0]*x**3 + p[1]*x**2 + p[2]*x + p[3]</code>) and the actual values</p><figure name="cb90" id="cb90" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WU38eJQbbohnSCaeiubNAg.png" data-width="578" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*WU38eJQbbohnSCaeiubNAg.png"></figure><h4 name="dc90" id="dc90" class="graf graf--h4 graf-after--figure">Nearest Neighbors:</h4><p name="c82c" id="c82c" class="graf graf--p graf-after--h4">Nearest neighbors is a simple and flexible machine-learning algorithm that can be used for classification and regression tasks. The basic idea is to predict the label or value of a new data point based on the labels or values of its nearest neighbors in the training set.</p><p name="2ae9" id="2ae9" class="graf graf--p graf-after--p">Given a set of data points (x₁, y₁), (x₂, y₂), …, (xₙ, yₙ), and a new data point x*, we want to predict the value of y* that corresponds to x*. Nearest neighbors do this by finding the k data points closest to x* and using their y values to make a prediction.</p><p name="1f9a" id="1f9a" class="graf graf--p graf-after--p">Let d(xᵢ, x*) be the distance between xᵢ and x*. Then the nearest neighbors solution is given by:</p><blockquote name="768a" id="768a" class="graf graf--blockquote graf-after--p">y* = 1/k ∑ yᵢ, where xᵢ is one of the k closest data points to x*</blockquote><p name="4daf" id="4daf" class="graf graf--p graf-after--blockquote">The choice of distance metric (e.g., Euclidean distance, Manhattan distance) can affect the accuracy of the predictions. Additionally, the value of k can also affect the performance of the model. A larger value of k will generally result in a smoother prediction, while a smaller value of k will result in a more variable prediction.</p><p name="bc4d" id="bc4d" class="graf graf--p graf-after--p">The algorithm first calculates the distances between the new point and all the points in the training set, using some metric such as Euclidean distance to make a prediction for a new data point. Then, it selects the k nearest points to the new point based on the distances, where k is a hyperparameter that needs to be set by the user. Finally, the algorithm predicts the label or value of the new point based on the labels or values of its k nearest neighbors, using some aggregation function such as majority vote for classification or average for regression</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="495d" id="495d" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br /><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br /><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br /><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br /><br /><span class="hljs-comment"># Load the iris dataset</span><br />iris = load_iris()<br />X, y = iris.data, iris.target<br /><br /><span class="hljs-comment"># Split the dataset into training and test sets</span><br />X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">42</span>)<br /><br /><span class="hljs-comment"># Fit a k-nearest neighbors classifier with k=5</span><br />knn = KNeighborsClassifier(n_neighbors=<span class="hljs-number">5</span>)<br />knn.fit(X_train, y_train)<br /><br /><span class="hljs-comment"># Make predictions on the test set</span><br />y_pred = knn.predict(X_test)<br /><br /><span class="hljs-comment"># Calculate the accuracy of the classifier</span><br />accuracy = accuracy_score(y_test, y_pred)<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy:&quot;</span>, accuracy)</span></pre><p name="f6eb" id="f6eb" class="graf graf--p graf-after--pre">In this example, we use the <code class="markup--code markup--p-code">KNeighborsClassifier</code> class from the <code class="markup--code markup--p-code">sklearn.neighbors</code> module to fit a k-nearest neighbors classifier with k=5 to the iris dataset. We split the dataset into training and test sets using the <code class="markup--code markup--p-code">train_test_split</code> function from the <code class="markup--code markup--p-code">sklearn.model_selection</code> module. We then fit the classifier on the training set and make predictions on the test set using the <code class="markup--code markup--p-code">predict</code> method of the classifier. Finally, we calculate the accuracy of the classifier using the <code class="markup--code markup--p-code">accuracy_score</code> function from the <code class="markup--code markup--p-code">sklearn.metrics</code> module.</p><p name="3f77" id="3f77" class="graf graf--p graf-after--p">One advantage of nearest neighbors is its flexibility — it can work well with small and large datasets. It can handle complex and nonlinear relationships between the features and the target variable. Another advantage is its robustness to outliers — since the prediction is based on the nearest neighbors, outliers are less likely to impact the prediction significantly.</p><p name="267a" id="267a" class="graf graf--p graf-after--p">However, there are also some limitations to the nearest neighbors algorithm. One limitation is its sensitivity to the choice of distance metric — different distance metrics can lead to different nearest neighbors and, therefore, different predictions. Another limitation is its tendency to overfit when the number of features is large since the high-dimensional feature space makes it more likely to have spurious correlations between the features and the target variable.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="ea10" id="ea10" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsRegressor<br /><br /><span class="hljs-comment"># Generate some random data</span><br />np.random.seed(<span class="hljs-number">0</span>)<br />x = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>)<br />y = np.sin(x) + np.random.normal(scale=<span class="hljs-number">0.1</span>, size=<span class="hljs-number">100</span>)<br /><br /><span class="hljs-comment"># Fit a K-nearest neighbors regression model to the data</span><br />k = <span class="hljs-number">5</span><br />model = KNeighborsRegressor(n_neighbors=k)<br />model.fit(x.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), y)<br />y_pred = model.predict(x.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br /><br /><span class="hljs-comment"># Plot the data and the fitted curve</span><br />fig, ax = plt.subplots(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))<br />ax.scatter(x, y, s=<span class="hljs-number">20</span>, alpha=<span class="hljs-number">0.9</span>, label=<span class="hljs-string">&#x27;Data&#x27;</span> , c = <span class="hljs-string">&#x27;black&#x27;</span>)<br />ax.plot(x, y_pred, c=<span class="hljs-string">&#x27;r&#x27;</span>, lw=<span class="hljs-number">2</span>, label=<span class="hljs-string">&#x27;KNN Regression&#x27;</span>)<br />ax.set_xlabel(<span class="hljs-string">&#x27;X&#x27;</span>, fontsize=<span class="hljs-number">14</span>)<br />ax.set_ylabel(<span class="hljs-string">&#x27;Y&#x27;</span>, fontsize=<span class="hljs-number">14</span>)<br />ax.set_title(<span class="hljs-string">f&#x27;K-Nearest Neighbors Regression (K=<span class="hljs-subst">{k}</span>)&#x27;</span>, fontsize=<span class="hljs-number">16</span>)<br />ax.legend(fontsize=<span class="hljs-number">12</span>)<br />plt.show()</span></pre><p name="153e" id="153e" class="graf graf--p graf-after--pre">We fit a K-nearest neighbors regression model to some noisy data in this plot. We’ve set the size of the data points to 20, the transparency to 0.9, and added a label to the scatter plot using the <code class="markup--code markup--p-code">label</code> parameter of the <code class="markup--code markup--p-code">scatter</code> function. We&#39;ve also set the color of the fitted curve to red, the linewidth to 2, and added a label to the line plot using the <code class="markup--code markup--p-code">label</code> parameter of the <code class="markup--code markup--p-code">plot</code> function. We&#39;ve added axis labels and a title using the <code class="markup--code markup--p-code">set_xlabel</code>, <code class="markup--code markup--p-code">set_ylabel</code>, and <code class="markup--code markup--p-code">set_title</code> functions and added a legend using the <code class="markup--code markup--p-code">legend</code> function. Finally, we&#39;ve adjusted the figure size using the <code class="markup--code markup--p-code">figsize</code> parameter of the <code class="markup--code markup--p-code">subplots</code> function.</p><figure name="7c25" id="7c25" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*wTSlPAP4qDJceZDthsIaSA.png" data-width="708" data-height="556" src="https://cdn-images-1.medium.com/max/800/1*wTSlPAP4qDJceZDthsIaSA.png"></figure><h4 name="b762" id="b762" class="graf graf--h4 graf-after--figure">Conclusion:</h4><p name="ac32" id="ac32" class="graf graf--p graf-after--h4">In this article, we discussed two simple yet powerful approaches for prediction in machine learning: least squares and nearest neighbors. Least squares is a linear regression technique that minimizes the sum of squared residuals. At the same time, the nearest neighbors predict the target variable based on the values of its nearest neighbors in the training set.</p><p name="eb76" id="eb76" class="graf graf--p graf-after--p">We explained how these approaches work, gave examples of their applications, discussed their strengths and weaknesses, and provided Python code examples. We also emphasized that the choice between these approaches depends on the specific problem and data set and that many other approaches to the prediction may be more appropriate for certain tasks.</p><p name="b45e" id="b45e" class="graf graf--p graf-after--p">Despite their simplicity, least squares and nearest neighbors are still powerful tools for prediction. They can handle a wide range of data types and are often used in practice for ease of implementation and interpretability. However, choosing the right method for a given problem requires careful consideration of the problem at hand, including the data&#39;s nature, the dataset&#39;s size, and the desired output.</p><p name="1316" id="1316" class="graf graf--p graf-after--p graf--trailing">While there are many different approaches to prediction in machine learning, least squares, and nearest neighbors remain important and useful tools that can be applied to a wide range of problems. By understanding the strengths and weaknesses of these methods and carefully considering each problem&#39;s specific requirements, data scientists can choose the best approach for their needs and achieve accurate and effective predictions.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@the_daft_introvert" class="p-author h-card">Vishal Sharma</a> on <a href="https://medium.com/p/e3fc3286b5b1"><time class="dt-published" datetime="2023-03-14T10:25:39.621Z">March 14, 2023</time></a>.</p><p><a href="https://medium.com/@the_daft_introvert/simple-yet-powerful-comparing-least-squares-and-nearest-neighbors-for-prediction-in-machine-e3fc3286b5b1" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 2, 2023.</p></footer></article></body></html>