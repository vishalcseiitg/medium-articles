<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>“Mastering K-Means Clustering: Tips, Tricks, and Pitfalls for Better Performance and Accuracy”</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">“Mastering K-Means Clustering: Tips, Tricks, and Pitfalls for Better Performance and Accuracy”</h1>
</header>
<section data-field="subtitle" class="p-summary">
Introduction:
</section>
<section data-field="body" class="e-content">
<section name="5168" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="b985" id="b985" class="graf graf--h3 graf--startsWithDoubleQuote graf--leading graf--title">“Mastering K-Means Clustering: Tips, Tricks, and Pitfalls for Better Performance and Accuracy”</h3><figure name="a35f" id="a35f" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*J6U-J9dDbUJFFZ43" data-width="3707" data-height="5561" data-unsplash-photo-id="oizbQGSlSA0" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*J6U-J9dDbUJFFZ43"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/ko/@contentpixie?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/ko/@contentpixie?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Content Pixie</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure><h4 name="745e" id="745e" class="graf graf--h4 graf-after--figure">Introduction:</h4><p name="e6ea" id="e6ea" class="graf graf--p graf-after--h4">Clustering is a fundamental technique in data analysis that involves grouping data points into similar clusters or groups based on certain characteristics or features. Clustering helps to identify patterns, relationships, and structures in data that may not be immediately apparent.</p><p name="9a8b" id="9a8b" class="graf graf--p graf-after--p">K-Means clustering is one of the most widely used clustering algorithms. An iterative algorithm partitions a dataset into K clusters, where K is a user-defined parameter. The algorithm assigns each data point to the nearest cluster centroid and then updates the centroid based on the mean of the points in the cluster. The algorithm continues to iterate until the centroids converge or the maximum number of iterations is reached.</p><p name="7d06" id="7d06" class="graf graf--p graf-after--p">This article aims to provide an overview of K-Means clustering, its applications, and best practices for using the algorithm. The article will discuss the basics of K-Means clustering, how to choose the right number of clusters, examples of real-world applications, tips for optimizing performance and accuracy, and common challenges and pitfalls. By the end of the article, readers should have a good understanding of K-Means clustering and be able to apply it to their own data analysis problems.</p><h4 name="cf02" id="cf02" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">K-Means Clustering Basics:</strong></h4><p name="f550" id="f550" class="graf graf--p graf-after--h4">K-Means clustering is a simple, efficient, unsupervised machine learning algorithm that partitions a given dataset into K clusters based on similarity. The basic principles of K-Means clustering are as follows:</p><ol class="postList"><li name="b5de" id="b5de" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Cluster: </strong>A cluster is a group of data points that are similar to each other based on certain characteristics or features.</li><li name="05eb" id="05eb" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Centroid:</strong> A centroid is a point in the data space representing a cluster&#39;s center. It is calculated as the mean of all the points in the cluster.</li><li name="1873" id="1873" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Distance metric:</strong> A distance metric is a function that measures the similarity or dissimilarity between two data points. The most commonly used distance metric in K-Means clustering is Euclidean distance.</li><li name="fe7d" id="fe7d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Iteration:</strong> K-Means clustering is an iterative algorithm that partitions the data into K clusters by repeatedly updating the cluster centroids and reassigning data points to the nearest centroid.</li></ol><p name="62d9" id="62d9" class="graf graf--p graf-after--li">Now, let’s illustrate the algorithm with a simple example:</p><p name="f62b" id="f62b" class="graf graf--p graf-after--p">Suppose we have a dataset of 10 data points with two features (x and y), as shown below:</p><blockquote name="bc42" id="bc42" class="graf graf--blockquote graf-after--p">(2,3), (3,4), (3,5), (4,5), (5,5), (5,6), (6,5), (7,7), (8,8), (9,9)</blockquote><p name="2d95" id="2d95" class="graf graf--p graf-after--blockquote">We want to partition the data into three clusters using K-Means clustering. Here’s how the algorithm works:</p><ol class="postList"><li name="2a2f" id="2a2f" class="graf graf--li graf-after--p">Choose K=3 random points in the data space as the initial centroids. Let’s say we choose (2,3), (5,5), and (9,9) as the initial centroids.</li><li name="f7e4" id="f7e4" class="graf graf--li graf-after--li">Assign each data point to the nearest centroid based on Euclidean distance. For example, (2,3) and (3,4) are assigned to the first centroid, (3,5), (4,5), (5,5), (5,6), and (6,5) are assigned to the second centroid, and (7,7), (8,8), and (9,9) are assigned to the third centroid.</li><li name="27f9" id="27f9" class="graf graf--li graf-after--li">Calculate the new centroids as the mean of the points in each cluster. For example, the new centroid for the first cluster is (2.5, 3.5), the new centroid for the second cluster is (4.6, 5.4), and the new centroid for the third cluster is (8, 8).</li><li name="3b2c" id="3b2c" class="graf graf--li graf-after--li">Repeat steps 2 and 3 until the centroids converge or the maximum number of iterations is reached.</li></ol><p name="1ed2" id="1ed2" class="graf graf--p graf-after--li">In this example, after a few iterations, the centroids converge and the final clusters are:</p><p name="7afd" id="7afd" class="graf graf--p graf-after--p">Cluster 1: (2,3), (3,4)</p><p name="5d2f" id="5d2f" class="graf graf--p graf-after--p">Cluster 2: (3,5), (4,5), (5,5), (5,6), (6,5)</p><p name="b07a" id="b07a" class="graf graf--p graf-after--p">Cluster 3: (7,7), (8,8), (9,9)</p><p name="7b44" id="7b44" class="graf graf--p graf-after--p">Thus, K-Means clustering has successfully partitioned the dataset into three clusters based on the similarity of the data points.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="dc74" id="dc74" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br /><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_blobs<br /><br /><span class="hljs-comment"># Generate sample dataset with 4 clusters</span><br />X, y = make_blobs(n_samples=<span class="hljs-number">300</span>, centers=<span class="hljs-number">4</span>, n_features=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">42</span>)<br /><br /><span class="hljs-comment"># Fit K-Means algorithm with 4 clusters</span><br />kmeans = KMeans(n_clusters=<span class="hljs-number">4</span>, init=<span class="hljs-string">&#x27;k-means++&#x27;</span>, max_iter=<span class="hljs-number">300</span>, n_init=<span class="hljs-number">10</span>, random_state=<span class="hljs-number">42</span>)<br />y_kmeans = kmeans.fit_predict(X)<br /><br /><span class="hljs-comment"># Plot clusters and centroids</span><br />plt.scatter(X[y_kmeans == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y_kmeans == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], s=<span class="hljs-number">30</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, label=<span class="hljs-string">&#x27;Cluster 1&#x27;</span> , alpha = <span class="hljs-number">0.6</span>)<br />plt.scatter(X[y_kmeans == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y_kmeans == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], s=<span class="hljs-number">30</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&#x27;Cluster 2&#x27;</span> , alpha = <span class="hljs-number">0.6</span>)<br />plt.scatter(X[y_kmeans == <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], X[y_kmeans == <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], s=<span class="hljs-number">30</span>, color=<span class="hljs-string">&#x27;green&#x27;</span>, label=<span class="hljs-string">&#x27;Cluster 3&#x27;</span> , alpha = <span class="hljs-number">0.6</span>)<br />plt.scatter(X[y_kmeans == <span class="hljs-number">3</span>, <span class="hljs-number">0</span>], X[y_kmeans == <span class="hljs-number">3</span>, <span class="hljs-number">1</span>], s=<span class="hljs-number">40</span>, color=<span class="hljs-string">&#x27;orange&#x27;</span>, label=<span class="hljs-string">&#x27;Cluster 4&#x27;</span> , alpha = <span class="hljs-number">0.6</span>)<br />plt.scatter(kmeans.cluster_centers_[:, <span class="hljs-number">0</span>], kmeans.cluster_centers_[:, <span class="hljs-number">1</span>], s=<span class="hljs-number">100</span>, color=<span class="hljs-string">&#x27;black&#x27;</span>, label=<span class="hljs-string">&#x27;Centroids&#x27;</span> , alpha = <span class="hljs-number">0.6</span>)<br />plt.title(<span class="hljs-string">&#x27;K-Means Clustering&#x27;</span>)<br />plt.xlabel(<span class="hljs-string">&#x27;Feature 1&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;Feature 2&#x27;</span>)<br />plt.legend()<br />plt.show()</span></pre><figure name="bf96" id="bf96" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*21YR5VFTQI2Z2k78qg1iQA.png" data-width="581" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*21YR5VFTQI2Z2k78qg1iQA.png"></figure><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="4d11" id="4d11" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br /><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_circles<br /><br /><span class="hljs-comment"># Generate sample dataset with 2 circles</span><br />X, y = make_circles(n_samples=<span class="hljs-number">1000</span>, factor=<span class="hljs-number">0.5</span>, noise=<span class="hljs-number">0.05</span>)<br /><br /><span class="hljs-comment"># Fit K-Means algorithm with 2 clusters</span><br />kmeans = KMeans(n_clusters=<span class="hljs-number">2</span>, init=<span class="hljs-string">&#x27;k-means++&#x27;</span>, max_iter=<span class="hljs-number">300</span>, n_init=<span class="hljs-number">10</span>, random_state=<span class="hljs-number">42</span>)<br />y_kmeans = kmeans.fit_predict(X)<br /><br /><span class="hljs-comment"># Plot clusters and centroids</span><br />plt.scatter(X[y_kmeans == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y_kmeans == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], s=<span class="hljs-number">30</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, label=<span class="hljs-string">&#x27;Cluster 1&#x27;</span> , alpha = <span class="hljs-number">0.6</span>)<br />plt.scatter(X[y_kmeans == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y_kmeans == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], s=<span class="hljs-number">30</span>, color=<span class="hljs-string">&#x27;green&#x27;</span>, label=<span class="hljs-string">&#x27;Cluster 2&#x27;</span> , alpha = <span class="hljs-number">0.6</span>)<br />plt.scatter(kmeans.cluster_centers_[:, <span class="hljs-number">0</span>], kmeans.cluster_centers_[:, <span class="hljs-number">1</span>], s=<span class="hljs-number">100</span>, color=<span class="hljs-string">&#x27;black&#x27;</span>, label=<span class="hljs-string">&#x27;Centroids&#x27;</span>)<br />plt.title(<span class="hljs-string">&#x27;K-Means Clustering on Circles&#x27;</span>)<br />plt.xlabel(<span class="hljs-string">&#x27;Feature 1&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;Feature 2&#x27;</span>)<br />plt.legend()<br />plt.show()</span></pre><figure name="3e1a" id="3e1a" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*MI2sswn1ROAJIWJKmdXI6w.png" data-width="579" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*MI2sswn1ROAJIWJKmdXI6w.png"></figure><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="8884" id="8884" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> mpl_toolkits.mplot3d <span class="hljs-keyword">import</span> Axes3D<br /><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br /><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_blobs<br /><br /><span class="hljs-comment"># Generate sample dataset with 3 blobs in 3D space</span><br />X, y = make_blobs(n_samples=<span class="hljs-number">1000</span>, n_features=<span class="hljs-number">3</span>, centers=<span class="hljs-number">3</span>, random_state=<span class="hljs-number">42</span>)<br /><br /><span class="hljs-comment"># Fit K-Means algorithm with 3 clusters</span><br />kmeans = KMeans(n_clusters=<span class="hljs-number">3</span>, init=<span class="hljs-string">&#x27;k-means++&#x27;</span>, max_iter=<span class="hljs-number">300</span>, n_init=<span class="hljs-number">10</span>, random_state=<span class="hljs-number">42</span>)<br />y_kmeans = kmeans.fit_predict(X)<br /><br /><span class="hljs-comment"># Plot clusters and centroids in 3D space</span><br />fig = plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br />ax = fig.add_subplot(<span class="hljs-number">111</span>, projection=<span class="hljs-string">&#x27;3d&#x27;</span>)<br />ax.scatter(X[y_kmeans == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y_kmeans == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], X[y_kmeans == <span class="hljs-number">0</span>, <span class="hljs-number">2</span>], s=<span class="hljs-number">30</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, label=<span class="hljs-string">&#x27;Cluster 1&#x27;</span> , alpha = <span class="hljs-number">0.6</span>)<br />ax.scatter(X[y_kmeans == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y_kmeans == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], X[y_kmeans == <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], s=<span class="hljs-number">30</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&#x27;Cluster 2&#x27;</span> , alpha = <span class="hljs-number">0.6</span>)<br />ax.scatter(X[y_kmeans == <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], X[y_kmeans == <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], X[y_kmeans == <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], s=<span class="hljs-number">30</span>, color=<span class="hljs-string">&#x27;green&#x27;</span>, label=<span class="hljs-string">&#x27;Cluster 3&#x27;</span> , alpha = <span class="hljs-number">0.6</span>)<br />ax.scatter(kmeans.cluster_centers_[:, <span class="hljs-number">0</span>], kmeans.cluster_centers_[:, <span class="hljs-number">1</span>], kmeans.cluster_centers_[:, <span class="hljs-number">2</span>], s=<span class="hljs-number">100</span>, color=<span class="hljs-string">&#x27;black&#x27;</span>, label=<span class="hljs-string">&#x27;Centroids&#x27;</span>)<br />ax.set_title(<span class="hljs-string">&#x27;K-Means Clustering in 3D Space&#x27;</span>)<br />ax.set_xlabel(<span class="hljs-string">&#x27;Feature 1&#x27;</span>)<br />ax.set_ylabel(<span class="hljs-string">&#x27;Feature 2&#x27;</span>)<br />ax.set_zlabel(<span class="hljs-string">&#x27;Feature 3&#x27;</span>)<br />ax.legend()<br />plt.show()</span></pre><figure name="0920" id="0920" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*8T2EqFcYbgkmP7IVNqDyhQ.png" data-width="666" data-height="656" src="https://cdn-images-1.medium.com/max/800/1*8T2EqFcYbgkmP7IVNqDyhQ.png"></figure><h4 name="69a8" id="69a8" class="graf graf--h4 graf-after--figure">Choosing the Right Number of Clusters:</h4><p name="83a4" id="83a4" class="graf graf--p graf-after--h4">Cluster validation is the process of evaluating the quality and reliability of the clusters obtained by a clustering algorithm. It is important because it helps to ensure that the clustering results are meaningful and useful for the intended analysis or application. Cluster validation methods can be used to compare different clustering algorithms, identify the optimal number of clusters, and assess the stability and consistency of the clustering results.</p><figure name="d34a" id="d34a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*JXe3WDJ4UnOiJtki" data-width="6016" data-height="4000" data-unsplash-photo-id="ToI01Apo4Pk" src="https://cdn-images-1.medium.com/max/800/0*JXe3WDJ4UnOiJtki"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@melpoole?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@melpoole?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Mel Poole</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure><p name="0e21" id="0e21" class="graf graf--p graf-after--figure">There are several methods for determining the optimal number of clusters, including the elbow method and the silhouette score.</p><ol class="postList"><li name="cad9" id="cad9" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Elbow method: </strong>The elbow method involves plotting the within-cluster sum of squares (WCSS) against the number of clusters. The WCSS is the sum of the squared distances between each point and its centroid within a cluster. As the number of clusters increases, the WCSS will decrease, but at some point, the rate of decrease will start to slow down, forming an “elbow” shape in the plot. The optimal number of clusters is the point at which the rate of decrease slows down significantly.</li></ol><p name="f3f7" id="f3f7" class="graf graf--p graf-after--li">For example, let’s say we have a dataset with 100 data points, and we apply K-Means clustering with K ranging from 1 to 10. We calculate the WCSS for each value of K and plot the results. The plot shows that the WCSS decreases rapidly as K increases from 1 to 3, but then the rate of decrease slows down. The elbow point occurs at K=3, indicating that 3 clusters are the optimal number for this dataset.</p><p name="bb28" id="bb28" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Silhouette score:</strong> The silhouette score is a measure of how well each data point fits into its assigned cluster compared to other clusters. It ranges from -1 to 1, where a score of 1 indicates that the data point is well-matched to its own cluster and poorly matched to other clusters, and a score of -1 indicates the opposite. The average silhouette score across all data points is used as a measure of the overall quality of the clustering.</p><p name="d286" id="d286" class="graf graf--p graf-after--p">For example, let’s say we have the same dataset as above and apply K-Means clustering with K ranging from 2 to 5. We calculate the silhouette score for each value of K and plot the results. The plot shows that the silhouette score is highest for K=3, indicating that 3 clusters are the optimal number for this dataset.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="60e7" id="60e7" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br /><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_blobs<br /><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> silhouette_score<br /><br /><span class="hljs-comment"># Generate sample dataset with 2 features and 4 clusters</span><br />X, y = make_blobs(n_samples=<span class="hljs-number">300</span>, centers=<span class="hljs-number">4</span>, n_features=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">42</span>)<br /><br /><span class="hljs-comment"># Elbow method</span><br />wcss = []<br /><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>):<br />    kmeans = KMeans(n_clusters=i, init=<span class="hljs-string">&#x27;k-means++&#x27;</span>, max_iter=<span class="hljs-number">300</span>, n_init=<span class="hljs-number">10</span>, random_state=<span class="hljs-number">42</span>)<br />    kmeans.fit(X)<br />    wcss.append(kmeans.inertia_)<br />plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>), wcss , c = <span class="hljs-string">&#x27;black&#x27;</span>)<br />plt.title(<span class="hljs-string">&#x27;Elbow Method&#x27;</span>)<br />plt.xlabel(<span class="hljs-string">&#x27;Number of Clusters&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;WCSS&#x27;</span>)<br />plt.show()<br /><br /><span class="hljs-comment"># Silhouette score</span><br />sil_scores = []<br /><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, <span class="hljs-number">11</span>):<br />    kmeans = KMeans(n_clusters=i, init=<span class="hljs-string">&#x27;k-means++&#x27;</span>, max_iter=<span class="hljs-number">300</span>, n_init=<span class="hljs-number">10</span>, random_state=<span class="hljs-number">42</span>)<br />    kmeans.fit(X)<br />    sil_scores.append(silhouette_score(X, kmeans.labels_))<br />plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, <span class="hljs-number">11</span>), sil_scores , c = <span class="hljs-string">&#x27;black&#x27;</span>)<br />plt.title(<span class="hljs-string">&#x27;Silhouette Score&#x27;</span>)<br />plt.xlabel(<span class="hljs-string">&#x27;Number of Clusters&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;Silhouette Score&#x27;</span>)<br />plt.show()</span></pre><figure name="ed53" id="ed53" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*kbHmQmAErz_pLYEdrXUf3g.png" data-width="589" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*kbHmQmAErz_pLYEdrXUf3g.png"></figure><figure name="17fc" id="17fc" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*NoiDVqLN5BkyNvIMbTpeqg.png" data-width="567" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*NoiDVqLN5BkyNvIMbTpeqg.png"></figure><p name="deb3" id="deb3" class="graf graf--p graf-after--figure">The code generates a sample dataset with 2 features and 4 clusters using the <code class="markup--code markup--p-code">make_blobs</code> function from scikit-learn. Then, it applies the K-Means algorithm with varying numbers of clusters (1 to 10) to calculate the within-cluster sum of squares (WCSS) for each number of clusters. The elbow method is then plotted to visualize the point of diminishing returns in terms of WCSS reduction.</p><p name="da9f" id="da9f" class="graf graf--p graf-after--p">Next, the code applies the K-Means algorithm with varying numbers of clusters (2 to 10) to calculate the silhouette score for each number of clusters. The silhouette score plot is then generated to visualize the optimal number of clusters based on the highest score.</p><h4 name="9316" id="9316" class="graf graf--h4 graf-after--p">Voronoi Diagram:</h4><p name="06aa" id="06aa" class="graf graf--p graf-after--h4">A Voronoi diagram is a geometric diagram that divides a plane into regions based on the distance to a set of points called generators. Each region contains all points that are closer to a particular generator than to any other generator.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="b6e8" id="b6e8" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">from</span> scipy.spatial <span class="hljs-keyword">import</span> Voronoi, voronoi_plot_2d<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><br /><span class="hljs-comment"># Generate some random points</span><br />points = np.random.rand(<span class="hljs-number">30</span>, <span class="hljs-number">2</span>)<br /><br /><span class="hljs-comment"># Compute the Voronoi diagram</span><br />vor = Voronoi(points)<br /><br /><br /><span class="hljs-comment"># Plot the Voronoi diagram</span><br />fig = voronoi_plot_2d(vor, show_vertices=<span class="hljs-literal">False</span>, line_colors=<span class="hljs-string">&#x27;green&#x27;</span>,<br />                      line_width=<span class="hljs-number">2</span>, line_alpha=<span class="hljs-number">0.8</span>, point_size=<span class="hljs-number">2</span>)<br /><br /><span class="hljs-comment"># Add the original points to the plot</span><br />plt.scatter(points[:,<span class="hljs-number">0</span>], points[:,<span class="hljs-number">1</span>], color=<span class="hljs-string">&#x27;black&#x27;</span>, s=<span class="hljs-number">20</span> , alpha = <span class="hljs-number">0.6</span>)<br /><br /><span class="hljs-comment"># Add labels and title to the plot</span><br />plt.xlabel(<span class="hljs-string">&#x27;X-axis&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;Y-axis&#x27;</span>)<br />plt.title(<span class="hljs-string">&#x27;Voronoi Diagram of Random Points&#x27;</span>)<br /><br /><span class="hljs-comment"># Set the aspect ratio to &#x27;equal&#x27; and remove the frame</span><br />plt.gca().set_aspect(<span class="hljs-string">&#x27;equal&#x27;</span>, adjustable=<span class="hljs-string">&#x27;box&#x27;</span>)<br />plt.gca().spines[<span class="hljs-string">&#x27;top&#x27;</span>].set_visible(<span class="hljs-literal">False</span>)<br />plt.gca().spines[<span class="hljs-string">&#x27;right&#x27;</span>].set_visible(<span class="hljs-literal">False</span>)<br />plt.gca().spines[<span class="hljs-string">&#x27;bottom&#x27;</span>].set_linewidth(<span class="hljs-number">0.5</span>)<br />plt.gca().spines[<span class="hljs-string">&#x27;left&#x27;</span>].set_linewidth(<span class="hljs-number">0.5</span>)<br /><br />plt.show()</span></pre><figure name="cb85" id="cb85" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*h9TNlhHdMagpi1rsQV0xyg.png" data-width="433" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*h9TNlhHdMagpi1rsQV0xyg.png"></figure><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="29b9" id="29b9" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> scipy.spatial <span class="hljs-keyword">import</span> Voronoi, voronoi_plot_2d<br /><br /><span class="hljs-comment"># generate random points</span><br />points = np.random.rand(<span class="hljs-number">100</span>, <span class="hljs-number">2</span>)<br /><br /><span class="hljs-comment"># compute Voronoi diagram</span><br />vor = Voronoi(points)<br /><br /><span class="hljs-comment"># plot Voronoi diagram</span><br />fig, ax = plt.subplots()<br />voronoi_plot_2d(vor, ax=ax)<br />plt.show()</span></pre><figure name="020b" id="020b" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*Sar2CbZ9qexZK4RP774Q9g.png" data-width="547" data-height="413" src="https://cdn-images-1.medium.com/max/800/1*Sar2CbZ9qexZK4RP774Q9g.png"></figure><h4 name="3b7d" id="3b7d" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Applications of K-Means Clustering:</strong></h4><p name="7dc7" id="7dc7" class="graf graf--p graf-after--h4">K-Means clustering is a widely used unsupervised learning algorithm with several real-world applications in various fields. Here are a few examples:</p><ol class="postList"><li name="4982" id="4982" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Customer Segmentation:</strong> K-Means clustering can be used to segment customers into different groups based on their purchasing behavior, demographics, and other attributes. This information can be used by businesses to tailor their marketing strategies and products to suit the needs of each group better.</li><li name="d08c" id="d08c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Image Compression:</strong> K-Means clustering can be used to compress images by grouping similar pixels together and replacing them with their centroid. This reduces the number of colors required to represent the image without losing too much detail.</li><li name="1c87" id="1c87" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Anomaly Detection:</strong> K-Means clustering can identify anomalies in datasets by identifying clusters with significantly different characteristics from the rest of the data. This can be useful in detecting fraud, network intrusions, or other abnormal events.</li></ol><p name="7184" id="7184" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Advantages of K-Means clustering:</strong></p><ul class="postList"><li name="0d1f" id="0d1f" class="graf graf--li graf-after--p">It is a fast and efficient algorithm that can handle large datasets.</li><li name="0ad8" id="0ad8" class="graf graf--li graf-after--li">It is easy to implement and widely used, with many libraries and tools to support it.</li><li name="c927" id="c927" class="graf graf--li graf-after--li">It can be effective at identifying relatively simple and well-defined clusters in data.</li></ul><p name="5d09" id="5d09" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Limitations of K-Means clustering:</strong></p><ul class="postList"><li name="1171" id="1171" class="graf graf--li graf-after--p">It assumes that clusters are spherical and equally sized, which may only sometimes be the case in real-world data.</li><li name="1a57" id="1a57" class="graf graf--li graf-after--li">It is sensitive to initial cluster centroids and may converge to local minima instead of the global minimum.</li><li name="768c" id="768c" class="graf graf--li graf-after--li">It only works well with datasets that have varying densities or irregular shapes, as it may create clusters that do not accurately represent the data.</li></ul><p name="957f" id="957f" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Compared to other clustering algorithms, K-Means has some advantages and limitations:</strong></p><ul class="postList"><li name="6d7a" id="6d7a" class="graf graf--li graf-after--p">Compared to hierarchical clustering, K-Means is faster and more scalable, but it requires the number of clusters to be specified in advance.</li><li name="abc9" id="abc9" class="graf graf--li graf-after--li">Compared to density-based clustering like DBSCAN, K-Means is simpler to implement and works well with large datasets, but it may struggle with datasets that have varying densities.</li><li name="92eb" id="92eb" class="graf graf--li graf-after--li">Compared to spectral clustering, K-Means is faster and more scalable, but it may not work well with non-linearly separable data.</li></ul><h4 name="ab1e" id="ab1e" class="graf graf--h4 graf-after--li">Future work:</h4><p name="ea88" id="ea88" class="graf graf--p graf-after--h4 graf--trailing">Possible directions for further research and development of K-Means clustering include improving its efficiency and scalability for larger datasets, developing new distance metrics to handle non-Euclidean data, and integrating it with other machine learning algorithms to improve its performance in various applications.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@the_daft_introvert" class="p-author h-card">Vishal Sharma</a> on <a href="https://medium.com/p/27d978e9b3af"><time class="dt-published" datetime="2023-03-15T12:02:27.778Z">March 15, 2023</time></a>.</p><p><a href="https://medium.com/@the_daft_introvert/mastering-k-means-clustering-tips-tricks-and-pitfalls-for-better-performance-and-accuracy-27d978e9b3af" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 2, 2023.</p></footer></article></body></html>