<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>“Unlocking the Power of Mahalanobis Distance: Exploring Multivariate Data Analysis with Python”</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">“Unlocking the Power of Mahalanobis Distance: Exploring Multivariate Data Analysis with Python”</h1>
</header>
<section data-field="subtitle" class="p-summary">
Introduction
</section>
<section data-field="body" class="e-content">
<section name="7777" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="0627" id="0627" class="graf graf--h3 graf--startsWithDoubleQuote graf--leading graf--title">“Unlocking the Power of Mahalanobis Distance: Exploring Multivariate Data Analysis with Python”</h3><figure name="0558" id="0558" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*NuxiVnl2fuc_O6vL" data-width="5472" data-height="3648" data-unsplash-photo-id="AT77Q0Njnt0" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*NuxiVnl2fuc_O6vL"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@isaacmsmith?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@isaacmsmith?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Isaac Smith</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure><p name="b2a1" id="b2a1" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Introduction</strong></p><p name="53b3" id="53b3" class="graf graf--p graf-after--p">Mahalanobis Distance is a statistical tool used to measure the distance between a point and a distribution. It is a powerful technique that considers the correlations between variables in a dataset, making it a valuable tool in various applications such as outlier detection, clustering, and classification.</p><p name="f35c" id="f35c" class="graf graf--p graf-after--p">For instance, let’s consider a scenario where a company wants to identify potential fraud in credit card transactions. The company collects data on various variables such as the transaction amount, location, time, and other credit card transaction details. It then uses Mahalanobis Distance to measure the distance between each transaction and the distribution of all trades. By doing this, it can identify transactions that are significantly different from the rest and may indicate fraudulent activity.</p><p name="7080" id="7080" class="graf graf--p graf-after--p">Mahalanobis Distance measures the distance between a point and a distribution, considering the correlations between variables in the data. It is the distance between a point x and a distribution with mean vector μ and covariance matrix Σ. The formula for Mahalanobis Distance is given as:</p><blockquote name="f376" id="f376" class="graf graf--blockquote graf-after--p">D² = (x-μ)ᵀΣ⁻¹(x-μ)</blockquote><p name="bd81" id="bd81" class="graf graf--p graf-after--blockquote">Where D² is the squared Mahalanobis Distance, x is the point in question, μ is the mean vector of the distribution, Σ is the covariance matrix of the distribution, and ᵀ denotes the transpose of a matrix.</p><p name="1a44" id="1a44" class="graf graf--p graf-after--p">To better understand this formula, let’s take an example. Suppose we have a dataset containing two variables, X and Y, and we want to measure the distance between a point (2, 3) and the distribution of all points in the dataset. We calculate the mean vector and covariance matrix of the dataset as follows:</p><p name="5865" id="5865" class="graf graf--p graf-after--p">μ = [mean(X), mean(Y)] = [3, 4] Σ = [[var(X), cov(X,Y)], [cov(X,Y), var(Y)]] = [[2, -1], [-1, 2]]</p><p name="0474" id="0474" class="graf graf--p graf-after--p">We can now use the Mahalanobis Distance formula to calculate the distance between the point (2, 3) and the distribution:</p><p name="226b" id="226b" class="graf graf--p graf-after--p">D² = ([2, 3]-[3, 4])ᵀ[[2, -1], [-1, 2]]⁻¹([2, 3]-[3, 4])</p><p name="e2f9" id="e2f9" class="graf graf--p graf-after--p">= [-1, -1]ᵀ[[2, -1], [-1, 2]]⁻¹[-1, -1]</p><p name="4d22" id="4d22" class="graf graf--p graf-after--p">= [2, -2]ᵀ[[2/3, 1/3], [1/3, 2/3]][2, -2]</p><p name="cb5c" id="cb5c" class="graf graf--p graf-after--p">= [2/3, -2/3]ᵀ[2, -2]</p><p name="9dff" id="9dff" class="graf graf--p graf-after--p">= 4/3.</p><p name="f7f0" id="f7f0" class="graf graf--p graf-after--p">Therefore, the squared Mahalanobis Distance between the point (2, 3) and the distribution is 4/3. By calculating the Mahalanobis Distance, we can determine how far the point is from the distribution, considering the correlations between the X and Y variables.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="6570" id="6570" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">from</span> scipy.spatial.distance <span class="hljs-keyword">import</span> mahalanobis<br /><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_blobs<br /><br /><span class="hljs-comment"># Create a dataset with 2 clusters</span><br />X, y = make_blobs(n_samples=<span class="hljs-number">100</span>, centers=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">42</span>)<br /><br /><span class="hljs-comment"># Calculate the mean vector and covariance matrix of the dataset</span><br />mu = np.mean(X, axis=<span class="hljs-number">0</span>)<br />sigma = np.cov(X.T)<br /><br /><span class="hljs-comment"># Calculate the Mahalanobis Distance between two points</span><br />x1 = [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>]<br />x2 = [-<span class="hljs-number">2</span>, -<span class="hljs-number">2</span>]<br />dist_x1 = mahalanobis(x1, mu, np.linalg.inv(sigma))<br />dist_x2 = mahalanobis(x2, mu, np.linalg.inv(sigma))<br /><br /><span class="hljs-comment"># Print the distances</span><br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Distance between point x1 and the distribution:&quot;</span>, dist_x1)<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Distance between point x2 and the distribution:&quot;</span>, dist_x2)<br /><br /><span class="hljs-comment">#OUTPUT</span><br /><br />Distance between point x1 <span class="hljs-keyword">and</span> the distribution: <span class="hljs-number">2.099478227196236</span><br />Distance between point x2 <span class="hljs-keyword">and</span> the distribution: <span class="hljs-number">8.065203145117373</span></span></pre><p name="d70e" id="d70e" class="graf graf--p graf-after--pre">Here are some examples of how Mahalanobis distance can be used:</p><ol class="postList"><li name="5f24" id="5f24" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Outlier detection:</strong> Mahalanobis distance can detect outliers in a dataset. Outliers are data points significantly different from the rest of the dataset. By calculating the Mahalanobis distance between each data point and the mean of the dataset, we can identify data points far away from the mean. These data points can be considered outliers and may need to be removed or investigated further.</li><li name="3eca" id="3eca" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Clustering:</strong> Mahalanobis distance can also be used for clustering data points. Clustering is the process of grouping similar data points together. By calculating the Mahalanobis distance between each data point and the mean of each cluster, we can determine which cluster a data point belongs to. This method is useful for clustering data points with different variances or covariances.</li><li name="5528" id="5528" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Image classification: </strong>Mahalanobis distance can be used in image classification tasks. This application uses Mahalanobis distance to measure the similarity between a test image and a set of training images. By calculating the Mahalanobis distance between the test image and each training image, we can determine which training image is most similar to the test image. This method is useful for tasks such as face recognition and object detection.</li><li name="6832" id="6832" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Fraud detection:</strong> Mahalanobis distance can be used for fraud detection in financial transactions. By calculating the Mahalanobis distance between a transaction and a set of historical transactions, we can determine if the transaction is unusual or suspicious. This method is useful for detecting fraudulent transactions that may otherwise go unnoticed.</li></ol><p name="bef5" id="bef5" class="graf graf--p graf-after--li">Here is an example of how Mahalanobis distance can be used to create beautiful plots for a real-life dataset:</p><p name="1a5f" id="1a5f" class="graf graf--p graf-after--p">For this example, let’s use the famous Iris dataset, which contains measurements for 150 Iris flowers. We will use the sepal length, width, and petal length as our features.</p><p name="4bc5" id="4bc5" class="graf graf--p graf-after--p">First, we will calculate the Mahalanobis distance for each data point in the dataset. We can do this using the following code in Python:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="94f6" id="94f6" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">from</span> scipy.spatial.distance <span class="hljs-keyword">import</span> mahalanobis<br /><br /><span class="hljs-comment"># load the iris dataset</span><br /><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br />iris = load_iris()<br /><br /><span class="hljs-comment"># calculate the mean and covariance matrix of the dataset</span><br />mean = np.mean(iris.data, axis=<span class="hljs-number">0</span>)<br />cov = np.cov(iris.data.T)<br /><br /><span class="hljs-comment"># calculate the Mahalanobis distance for each data point</span><br />mahalanobis_dist = [mahalanobis(x, mean, np.linalg.inv(cov)) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> iris.data]</span></pre><p name="e6f7" id="e6f7" class="graf graf--p graf-after--pre">Next, we can create a scatter plot of the iris dataset using the first two features (sepal length and sepal width) and color each data point based on its Mahalanobis distance. We can use a color map to map the Mahalanobis distances to a color scale. Here’s the code for the plot:</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="f837" id="f837" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> ListedColormap<br /><br /><span class="hljs-comment"># create a color map for the Mahalanobis distances</span><br />cmap = ListedColormap([<span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>])<br />norm = plt.Normalize(<span class="hljs-built_in">min</span>(mahalanobis_dist), <span class="hljs-built_in">max</span>(mahalanobis_dist))<br /><br /><span class="hljs-comment"># create a scatter plot of the iris dataset</span><br />plt.scatter(iris.data[:, <span class="hljs-number">0</span>], iris.data[:, <span class="hljs-number">1</span>], c=mahalanobis_dist, cmap=cmap, norm=norm)<br /><br /><span class="hljs-comment"># add a color bar</span><br />plt.colorbar()<br />plt.xlabel(<span class="hljs-string">&#x27;sepal length&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;sepal width&#x27;</span>)<br />plt.title(<span class="hljs-string">&#x27;Mahalanobis Distance for Iris Dataset&#x27;</span>)<br />plt.show()</span></pre><figure name="c8ab" id="c8ab" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*o6G0ke6D2vPX6gnbpk0lTg.png" data-width="543" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*o6G0ke6D2vPX6gnbpk0lTg.png"><figcaption class="imageCaption">The resulting plot shows the Iris dataset with each data point colored based on its Mahalanobis distance. Data points that are far away from the mean (i.e., outliers) are colored in red, while data points that are closer to the mean are colored in green and blue.</figcaption></figure><p name="e376" id="e376" class="graf graf--p graf-after--figure">Here is another example using the famous Wine dataset that comes with the sci-kit-learn library:</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="7e2b" id="7e2b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">from</span> scipy.spatial.distance <span class="hljs-keyword">import</span> mahalanobis<br /><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br /><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_wine<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> ListedColormap<br /><br /><span class="hljs-comment"># load the wine dataset</span><br />wine = load_wine()<br />wine_df = pd.DataFrame(wine.data, columns=wine.feature_names)<br /><br /><span class="hljs-comment"># calculate the mean and covariance matrix of the dataset</span><br />mean = np.mean(wine_df, axis=<span class="hljs-number">0</span>)<br />cov = np.cov(wine_df.T)<br /><br /><span class="hljs-comment"># calculate the Mahalanobis distance for each data point</span><br />mahalanobis_dist = [mahalanobis(x, mean, np.linalg.inv(cov)) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> wine_df.values]<br /><br /><span class="hljs-comment"># create a scatter plot of the wine dataset using two highly correlated features</span><br />plt.scatter(wine_df[<span class="hljs-string">&#x27;flavanoids&#x27;</span>], wine_df[<span class="hljs-string">&#x27;color_intensity&#x27;</span>], c=mahalanobis_dist, cmap=<span class="hljs-string">&#x27;coolwarm&#x27;</span>)<br /><br /><span class="hljs-comment"># add a color bar</span><br />plt.colorbar()<br />plt.xlabel(<span class="hljs-string">&#x27;flavanoids&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;color_intensity&#x27;</span>)<br />plt.title(<span class="hljs-string">&#x27;Mahalanobis Distance for Wine Dataset&#x27;</span>)<br />plt.show()</span></pre><figure name="9a64" id="9a64" class="graf graf--figure graf-after--pre graf--trailing"><img class="graf-image" data-image-id="1*VMOX9ZwTY4gyWAJSB5Ea4w.png" data-width="525" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*VMOX9ZwTY4gyWAJSB5Ea4w.png"><figcaption class="imageCaption">A scatter plot of two highly correlated features (flavonoids and color_intensity) with each data point colored based on its Mahalanobis distance.</figcaption></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@the_daft_introvert" class="p-author h-card">Vishal Sharma</a> on <a href="https://medium.com/p/5c11a757b099"><time class="dt-published" datetime="2023-03-06T18:14:49.833Z">March 6, 2023</time></a>.</p><p><a href="https://medium.com/@the_daft_introvert/mahalanobis-distance-5c11a757b099" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 2, 2023.</p></footer></article></body></html>