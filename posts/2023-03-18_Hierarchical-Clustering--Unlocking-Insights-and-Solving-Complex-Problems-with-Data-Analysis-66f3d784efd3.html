<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Hierarchical Clustering: Unlocking Insights and Solving Complex Problems with Data Analysis</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Hierarchical Clustering: Unlocking Insights and Solving Complex Problems with Data Analysis</h1>
</header>
<section data-field="subtitle" class="p-summary">
In the age of big data, making sense of vast amounts of information can be a daunting task. However, with the help of advanced data…
</section>
<section data-field="body" class="e-content">
<section name="6b9e" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="21f4" id="21f4" class="graf graf--h3 graf--leading graf--title">Hierarchical Clustering: Unlocking Insights and Solving Complex Problems with Data Analysis</h3><blockquote name="24c5" id="24c5" class="graf graf--pullquote graf-after--h3">In the age of big data, making sense of vast amounts of information can be a daunting task. However, with the help of advanced data analysis techniques such as hierarchical clustering, we can unlock valuable insights and solve complex problems that were once considered too challenging to tackle.</blockquote><figure name="7348" id="7348" class="graf graf--figure graf-after--pullquote"><img class="graf-image" data-image-id="0*FdG7rlbkvKC586G0" data-width="2674" data-height="4009" data-unsplash-photo-id="IHtVbLRjTZU" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*FdG7rlbkvKC586G0"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@filipkominik?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@filipkominik?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Filip Kominik</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure><p name="a071" id="a071" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--figure"><span class="graf-dropCap">C</span>lustering is a technique in data analysis that involves grouping similar data points based on their similarity or distance. Clustering helps to identify patterns and structures in data that may not be readily apparent through other means, such as visualization or summary statistics. It is widely used in marketing, biology, finance, and image processing.</p><p name="649f" id="649f" class="graf graf--p graf-after--p graf--trailing">Hierarchical clustering is a clustering algorithm that groups data points into a hierarchy of clusters based on their similarity. In this algorithm, clusters are merged or divided recursively until all data points belong to a single cluster. Hierarchical clustering differs from other clustering algorithms in that it allows for the identification of clusters at different levels of granularity. Agglomerative hierarchical clustering is the most common type of hierarchical clustering, in which small clusters are successively merged to form larger ones. Divisive hierarchical clustering, on the other hand, starts with one big cluster and recursively divides it into smaller ones.</p></div></div></section><section name="9a5b" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><blockquote name="60ee" id="60ee" class="graf graf--pullquote graf--leading">There are two main types of hierarchical clustering: agglomerative and divisive.</blockquote><p name="f7e8" id="f7e8" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--pullquote"><span class="graf-dropCap">A</span>gglomerative hierarchical clustering is the most common type. It involves starting with each data point as its own cluster and iteratively merging the two closest clusters until all data points belong to a single cluster. The steps involved in agglomerative hierarchical clustering are as follows:</p><ol class="postList"><li name="2115" id="2115" class="graf graf--li graf-after--p">Start with each data point as its own cluster.</li><li name="2210" id="2210" class="graf graf--li graf-after--li">Compute the distance between all pairs of clusters.</li><li name="9c9f" id="9c9f" class="graf graf--li graf-after--li">Merge the two closest clusters into a new cluster.</li><li name="7975" id="7975" class="graf graf--li graf-after--li">Recompute the distance between the new cluster and all other clusters.</li><li name="50b1" id="50b1" class="graf graf--li graf-after--li">Repeat steps 3 and 4 until all data points belong to one cluster.</li></ol><p name="a7fd" id="a7fd" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--li"><span class="graf-dropCap">D</span>ivisive hierarchical clustering, on the other hand, starts with one big cluster containing all data points and recursively divides it into smaller clusters until each data point is in its own cluster. The steps involved in divisive hierarchical clustering are as follows:</p><ol class="postList"><li name="09c8" id="09c8" class="graf graf--li graf-after--p">Start with all data points in one big cluster.</li><li name="25ad" id="25ad" class="graf graf--li graf-after--li">Compute the distance between all pairs of data points.</li><li name="3956" id="3956" class="graf graf--li graf-after--li">Identify the data point or points that are the most dissimilar from the others.</li><li name="23b0" id="23b0" class="graf graf--li graf-after--li">Split the cluster into two smaller clusters based on the dissimilar data point(s).</li><li name="ed1f" id="ed1f" class="graf graf--li graf-after--li">Recursively repeat steps 2–4 on each of the smaller clusters until each data point is in its own cluster.</li></ol><blockquote name="b47d" id="b47d" class="graf graf--blockquote graf-after--li graf--trailing">Agglomerative hierarchical clustering starts with each data point as its own cluster. It merges them iteratively, while divisive hierarchical clustering starts with one big cluster and recursively divides it into smaller clusters.</blockquote></div></div></section><section name="a4c8" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="c8b7" id="c8b7" class="graf graf--p graf--leading">The choice of distance metric in hierarchical clustering is a critical factor that can significantly impact the results of the clustering algorithm. A distance metric is used to compute the distance between two data points or clusters, defining the notion of similarity or dissimilarity between them. Different distance metrics can lead to different clustering results as they measure the distance between data points differently.</p><p name="26e3" id="26e3" class="graf graf--p graf-after--p">Several distance metrics can be used in hierarchical clustering, including Euclidean distance, Manhattan distance, and cosine distance. Euclidean distance is the most commonly used distance metric and measures the straight-line distance between two data points in n-dimensional space. Manhattan distance, also known as taxicab distance, measures the distance between two points by summing the absolute differences between their coordinates. Cosine distance measures the angle between two vectors in n-dimensional space and is commonly used in text mining and information retrieval.</p><p name="374b" id="374b" class="graf graf--p graf-after--p graf--trailing">The choice of distance metric should be based on the nature of the data being analyzed and the problem being solved. For example, Euclidean distance is appropriate for continuous data, while Manhattan distance is more suitable for discrete data. Cosine distance is commonly used in text mining because it is robust to document length differences and is insensitive to the frequency of terms.</p></div></div></section><section name="4371" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><blockquote name="41f3" id="41f3" class="graf graf--pullquote graf--leading">Dendrograms are graphical representations of the results of hierarchical clustering that show the hierarchy of clusters formed by the algorithm. They are commonly used to visualize and interpret the results of hierarchical clustering.</blockquote><p name="6720" id="6720" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--pullquote"><span class="graf-dropCap">D</span>endrogram is a tree-like diagram in which each leaf represents a data point, and each internal node represents a cluster. The height of each internal node corresponds to the distance between the two clusters being merged at that step. The vertical axis of the dendrogram represents the distance or dissimilarity between the data points or clusters, while the horizontal axis represents the individual data points or clusters.</p><p name="171b" id="171b" class="graf graf--p graf-after--p">To read a dendrogram, one should start from the bottom, where the individual data points are shown, and follow the lines up to the top of the tree. The height at which the lines are merged into clusters indicates the distance or dissimilarity between the clusters being merged. The height of each node represents the distance between the clusters at that point in the tree.</p><p name="cefa" id="cefa" class="graf graf--p graf-after--p">Interpreting the results of a dendrogram involves identifying the different levels of granularity at which the clusters are formed. The height at which the clusters are merged indicates the similarity or dissimilarity between them, and the number of clusters formed at each level can provide insight into the underlying structure of the data.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="6abd" id="6abd" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">from</span> scipy.cluster.hierarchy <span class="hljs-keyword">import</span> dendrogram, linkage<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><br /><span class="hljs-comment"># Generate sample data</span><br />np.random.seed(<span class="hljs-number">0</span>)<br />X = np.random.randn(<span class="hljs-number">10</span>, <span class="hljs-number">2</span>)<br /><br /><span class="hljs-comment"># Perform hierarchical clustering</span><br />Z = linkage(X, method=<span class="hljs-string">&#x27;ward&#x27;</span>)<br /><br /><span class="hljs-comment"># Customizing dendrogram plot</span><br />plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">5</span>))<br />plt.title(<span class="hljs-string">&#x27;Hierarchical Clustering Dendrogram&#x27;</span>)<br />plt.xlabel(<span class="hljs-string">&#x27;Sample Index&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;Distance&#x27;</span>)<br />dendrogram(Z, leaf_rotation=<span class="hljs-number">90.</span>, leaf_font_size=<span class="hljs-number">8.</span>, color_threshold=<span class="hljs-number">0.3</span>*<span class="hljs-built_in">max</span>(Z[:,<span class="hljs-number">2</span>]), above_threshold_color=<span class="hljs-string">&#x27;gray&#x27;</span>)<br />plt.axhline(y=<span class="hljs-number">0.3</span>*<span class="hljs-built_in">max</span>(Z[:,<span class="hljs-number">2</span>]), color=<span class="hljs-string">&#x27;r&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br />plt.show()</span></pre><figure name="a94f" id="a94f" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*qFhi--c5mKc465I3tfOuZQ.png" data-width="846" data-height="461" src="https://cdn-images-1.medium.com/max/800/1*qFhi--c5mKc465I3tfOuZQ.png"></figure><p name="a6ad" id="a6ad" class="graf graf--p graf-after--figure graf--trailing">Dendrograms have several advantages for analyzing the results of hierarchical clustering. They visually represent the clustering results, making it easier to identify the clusters and understand their relationships. Dendrograms can also help to determine the appropriate number of clusters to use in subsequent analyses, as they allow the user to identify the optimal level of clustering based on the underlying data structure. Finally, dendrograms can be used to compare the results of different clustering algorithms or distance metrics, enabling users to choose the best approach for their specific problem.</p></div></div></section><section name="534c" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><blockquote name="d130" id="d130" class="graf graf--pullquote graf--leading">Data Visualization on the Iris Datasets:</blockquote><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="6949" id="6949" class="graf graf--pre graf-after--pullquote graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> scipy.cluster.hierarchy <span class="hljs-keyword">import</span> dendrogram, linkage<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br /><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br /><br /><span class="hljs-comment"># Load iris dataset</span><br />iris = sns.load_dataset(<span class="hljs-string">&#x27;iris&#x27;</span>)<br /><br /><span class="hljs-comment"># Extract features</span><br />X = iris.iloc[:, :-<span class="hljs-number">1</span>]<br /><br /><span class="hljs-comment"># Perform hierarchical clustering using Ward&#x27;s method and Euclidean distance</span><br />Z = linkage(X, method=<span class="hljs-string">&#x27;ward&#x27;</span>, metric=<span class="hljs-string">&#x27;euclidean&#x27;</span>)<br /><br /><span class="hljs-comment"># Plot dendrogram</span><br />plt.figure(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>))<br />plt.title(<span class="hljs-string">&#x27;Iris Hierarchical Clustering Dendrogram&#x27;</span>)<br />plt.xlabel(<span class="hljs-string">&#x27;Sample Index&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;Distance&#x27;</span>)<br />dendrogram(Z, leaf_rotation=<span class="hljs-number">90.</span>, leaf_font_size=<span class="hljs-number">8.</span>)<br />plt.show()</span></pre><p name="6eb3" id="6eb3" class="graf graf--p graf-after--pre">This code generates a dendrogram plot for the iris dataset using hierarchical clustering with Ward’s method and Euclidean distance. The resulting plot shows the hierarchical structure of the clusters formed by the algorithm, with the distance between clusters indicated by the height of the branches.</p><figure name="8da2" id="8da2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*X336oO-DAsR-NCs9szcicw.png" data-width="1164" data-height="1153" src="https://cdn-images-1.medium.com/max/800/1*X336oO-DAsR-NCs9szcicw.png"></figure><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="scss" name="0aa7" id="0aa7" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"># Add species labels to the dendrogram<br />def <span class="hljs-built_in">fancy_dendrogram</span>(*args, **kwargs):<br />    max_d = kwargs.<span class="hljs-built_in">pop</span>(<span class="hljs-string">&#x27;max_d&#x27;</span>, None)<br />    if max_d and <span class="hljs-string">&#x27;color_threshold&#x27;</span> not in kwargs:<br />        kwargs[<span class="hljs-string">&#x27;color_threshold&#x27;</span>] = max_d<br />    annotate_above = kwargs.<span class="hljs-built_in">pop</span>(<span class="hljs-string">&#x27;annotate_above&#x27;</span>, <span class="hljs-number">0</span>)<br />    ddata = <span class="hljs-built_in">dendrogram</span>(*args, **kwargs)<br />    if not kwargs.<span class="hljs-built_in">get</span>(<span class="hljs-string">&#x27;no_plot&#x27;</span>, False):<br />        plt.<span class="hljs-built_in">title</span>(<span class="hljs-string">&#x27;Iris Hierarchical Clustering Dendrogram&#x27;</span>)<br />        plt.<span class="hljs-built_in">xlabel</span>(<span class="hljs-string">&#x27;Sample Index&#x27;</span>)<br />        plt.<span class="hljs-built_in">ylabel</span>(<span class="hljs-string">&#x27;Distance&#x27;</span>)<br />        for i, d, c in <span class="hljs-built_in">zip</span>(ddata[<span class="hljs-string">&#x27;icoord&#x27;</span>], ddata[<span class="hljs-string">&#x27;dcoord&#x27;</span>], ddata[<span class="hljs-string">&#x27;color_list&#x27;</span>]):<br />            x = <span class="hljs-number">0.5</span> * <span class="hljs-built_in">sum</span>(i[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>])<br />            y = d[<span class="hljs-number">1</span>]<br />            if y &gt; annotate_above:<br />                plt.<span class="hljs-built_in">plot</span>(x, y, <span class="hljs-string">&#x27;o&#x27;</span>, c=c)<br />                plt.<span class="hljs-built_in">annotate</span>(<span class="hljs-string">&quot;%.2g&quot;</span> % y, (x, y), xytext=(<span class="hljs-number">0</span>, -<span class="hljs-number">5</span>),<br />                             textcoords=<span class="hljs-string">&#x27;offset points&#x27;</span>,<br />                             va=<span class="hljs-string">&#x27;top&#x27;</span>, ha=<span class="hljs-string">&#x27;center&#x27;</span>)<br />    if max_d:<br />        plt.<span class="hljs-built_in">axhline</span>(y=max_d, c=<span class="hljs-string">&#x27;k&#x27;</span>)<br />    return ddata<br /><br /># Plot dendrogram with species labels<br />plt.<span class="hljs-built_in">figure</span>(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>))<br /><span class="hljs-built_in">fancy_dendrogram</span>(Z, labels=iris[<span class="hljs-string">&#x27;species&#x27;</span>].values, max_d=<span class="hljs-number">7.0</span>, leaf_rotation=<span class="hljs-number">90</span>., leaf_font_size=<span class="hljs-number">8</span>.)<br />plt.<span class="hljs-built_in">show</span>()</span></pre><p name="726b" id="726b" class="graf graf--p graf-after--pre">This code generates a dendrogram plot with color-coded points indicating the species of each sample. The <code class="markup--code markup--p-code">fancy_dendrogram</code> function is used to annotate the plot with the species labels, and a horizontal line is added to indicate a threshold distance for splitting the dendrogram into clusters.</p><figure name="8c6d" id="8c6d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6O8qcAYNmFNABGBI4BJeXg.png" data-width="1167" data-height="1178" src="https://cdn-images-1.medium.com/max/800/1*6O8qcAYNmFNABGBI4BJeXg.png"></figure><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="ruby" name="116c" id="116c" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"><span class="hljs-comment"># Load iris dataset</span><br />iris = sns.load_dataset(<span class="hljs-string">&#x27;iris&#x27;</span>)<br /><br /><span class="hljs-comment"># Extract features</span><br />X = iris.iloc[<span class="hljs-symbol">:</span>, <span class="hljs-symbol">:-</span><span class="hljs-number">1</span>]<br /><br /><span class="hljs-comment"># Perform hierarchical clustering using different linkage methods and distance metrics</span><br />Z_single = linkage(X, method=<span class="hljs-string">&#x27;single&#x27;</span>, metric=<span class="hljs-string">&#x27;euclidean&#x27;</span>)<br />Z_complete = linkage(X, method=<span class="hljs-string">&#x27;complete&#x27;</span>, metric=<span class="hljs-string">&#x27;euclidean&#x27;</span>)<br />Z_average = linkage(X, method=<span class="hljs-string">&#x27;average&#x27;</span>, metric=<span class="hljs-string">&#x27;euclidean&#x27;</span>)<br /><br /><span class="hljs-comment"># Plot dendrograms for each clustering method</span><br />plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">5</span>))<br />plt.suptitle(<span class="hljs-string">&#x27;Iris Hierarchical Clustering Dendrograms&#x27;</span>)<br />plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br />plt.title(<span class="hljs-string">&#x27;Single Linkage&#x27;</span>)<br />dendrogram(Z_single, leaf_rotation=<span class="hljs-number">90</span>., leaf_font_size=<span class="hljs-number">8</span>.)<br />plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br />plt.title(<span class="hljs-string">&#x27;Complete Linkage&#x27;</span>)<br />dendrogram(Z_complete, leaf_rotation=<span class="hljs-number">90</span>., leaf_font_size=<span class="hljs-number">8</span>.)<br />plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br />plt.title(<span class="hljs-string">&#x27;Average Linkage&#x27;</span>)<br />dendrogram(Z_average, leaf_rotation=<span class="hljs-number">90</span>., leaf_font_size=<span class="hljs-number">8</span>.)<br />plt.tight_layout()<br />plt.show()</span></pre><p name="0417" id="0417" class="graf graf--p graf-after--pre">This code generates three dendrogram plots for the iris dataset using different linkage methods and Euclidean distance. The resulting plots show the different hierarchical structures formed by each method.</p><figure name="552c" id="552c" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*eOArEgg2PLqEygLzf9YtRQ.png" data-width="1432" data-height="1414" src="https://cdn-images-1.medium.com/max/800/1*eOArEgg2PLqEygLzf9YtRQ.png"></figure></div></div></section><section name="b01c" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><blockquote name="389f" id="389f" class="graf graf--pullquote graf--leading">Hierarchical clustering has a wide range of applications across various fields. Here are some common examples of how hierarchical clustering is used in different domains:</blockquote><ol class="postList"><li name="4f97" id="4f97" class="graf graf--li graf-after--pullquote"><strong class="markup--strong markup--li-strong">Biology</strong>: Hierarchical clustering is often used in biology to group genes or proteins based on their expression profiles and identify gene expression patterns across different samples. For example, hierarchical clustering has been used to classify breast cancer tumors based on their molecular profiles, to identify different subtypes of leukemia, and to study the evolution of viruses.</li><li name="b2d5" id="b2d5" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Marketing</strong>: Hierarchical clustering can be used in market segmentation to group customers with similar purchasing behavior, demographics, and other characteristics. For example, a company could use hierarchical clustering to segment its customers based on their purchasing history and preferences and then target each segment with specific marketing campaigns.</li><li name="9a15" id="9a15" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Finance</strong>: Hierarchical clustering can be used in finance to identify groups of stocks or other financial instruments that have similar risk and return characteristics. For example, a portfolio manager could use hierarchical clustering to group stocks based on their market capitalization, sector, and other factors and then construct a diversified portfolio with stocks from different clusters.</li></ol><blockquote name="3380" id="3380" class="graf graf--pullquote graf-after--li">Some specific examples of how hierarchical clustering has been used to solve real-world problems:</blockquote><ol class="postList"><li name="9fc3" id="9fc3" class="graf graf--li graf-after--pullquote"><strong class="markup--strong markup--li-strong">Identifying customer segments:</strong> A grocery store chain uses hierarchical clustering to group customers based on their purchasing history, demographic information, and location. They were able to identify several distinct customer segments, each with its own preferences and buying habits. This allowed the store to tailor their marketing campaigns and product offerings to each segment.</li><li name="1bd8" id="1bd8" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Identifying disease subtypes:</strong> Researchers used hierarchical clustering to analyze gene expression data from breast cancer tumors. They identified several distinct subtypes of breast cancer, each with its own molecular profile and prognosis. This helped doctors to classify better and treat breast cancer patients based on their individual subtypes.</li><li name="0b1a" id="0b1a" class="graf graf--li graf-after--li graf--trailing"><strong class="markup--strong markup--li-strong">Portfolio optimization:</strong> A portfolio manager uses hierarchical clustering to group stocks based on their industry, market capitalization, and other factors. They were able to construct a diversified portfolio with stocks from different clusters, which reduced the overall risk of the portfolio and improved returns.</li></ol></div></div></section><section name="c638" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><blockquote name="4061" id="4061" class="graf graf--pullquote graf--leading">Conclusion:</blockquote><p name="18f2" id="18f2" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--pullquote"><span class="graf-dropCap">H</span>ierarchical clustering is an important technique in data analysis that involves grouping data points into hierarchical structures. It can be used for a wide range of applications across various fields, such as biology, marketing, and finance. The two types of hierarchical clustering are agglomerative and divisive. The choice of distance metric can affect the results of hierarchical clustering, and it’s important to select an appropriate distance metric based on the nature of the data being analyzed. Dendrograms are used to visualize the results of hierarchical clustering, which allows for the interpretation of the clustering results. Hierarchical clustering has the potential to solve complex problems such as identifying customer segments, identifying disease subtypes, and optimizing portfolios. Overall, hierarchical clustering is a powerful tool for analyzing large datasets and identifying meaningful patterns and groups, and it has a wide range of applications in solving real-world problems.</p><blockquote name="9730" id="9730" class="graf graf--pullquote graf-after--p">References:</blockquote><ol class="postList"><li name="ec3a" id="ec3a" class="graf graf--li graf--startsWithDoubleQuote graf-after--pullquote">“Pattern Recognition and Machine Learning” by Christopher M. Bishop</li><li name="799b" id="799b" class="graf graf--li graf--startsWithDoubleQuote graf-after--li">“Data Mining: Concepts and Techniques” by Jiawei Han, Micheline Kamber, and Jian Pei</li><li name="ca25" id="ca25" class="graf graf--li graf--startsWithDoubleQuote graf-after--li">“Introduction to Data Mining” by Pang-Ning Tan, Michael Steinbach, and Vipin Kumar</li><li name="ce2b" id="ce2b" class="graf graf--li graf--startsWithDoubleQuote graf-after--li graf--trailing">“Multivariate Analysis for the Biobehavioral and Social Sciences: A Graphical Approach” by Bruce Thompson</li></ol></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@the_daft_introvert" class="p-author h-card">Vishal Sharma</a> on <a href="https://medium.com/p/66f3d784efd3"><time class="dt-published" datetime="2023-03-18T14:35:19.131Z">March 18, 2023</time></a>.</p><p><a href="https://medium.com/@the_daft_introvert/hierarchical-clustering-unlocking-insights-and-solving-complex-problems-with-data-analysis-66f3d784efd3" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 2, 2023.</p></footer></article></body></html>