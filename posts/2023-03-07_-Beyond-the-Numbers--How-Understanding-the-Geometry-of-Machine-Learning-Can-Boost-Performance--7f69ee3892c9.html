<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>“Beyond the Numbers: How Understanding the Geometry of Machine Learning Can Boost Performance”</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">“Beyond the Numbers: How Understanding the Geometry of Machine Learning Can Boost Performance”</h1>
</header>
<section data-field="subtitle" class="p-summary">
Machine learning algorithms are widely used in various fields and have revolutionized how we approach data analysis. These algorithms are…
</section>
<section data-field="body" class="e-content">
<section name="8991" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="a0fc" id="a0fc" class="graf graf--h3 graf--startsWithDoubleQuote graf--leading graf--title">“Beyond the Numbers: How Understanding the Geometry of Machine Learning Can Boost Performance”</h3><p name="f962" id="f962" class="graf graf--p graf-after--h3">Machine learning algorithms are widely used in various fields and have revolutionized how we approach data analysis. These algorithms are based on mathematical models and often operate in high-dimensional spaces, making it challenging to interpret their behavior. However, understanding the geometry of these models can provide important insights into how they work and how they can be optimized to improve performance.</p><p name="f39c" id="f39c" class="graf graf--p graf-after--p">In this article, we will explore the importance of geometry in machine learning algorithms. We will discuss key geometric concepts, such as distance, angle, and convexity, and how they are relevant to various machine learning algorithms. We will also provide examples of how geometric intuition can be used to optimize and gain insights into machine learning algorithms, such as support vector machines (SVMs), and principal component analysis (PCA).</p><p name="2f30" id="2f30" class="graf graf--p graf-after--p">Overall, this article provides a comprehensive overview of the role of geometry in machine learning algorithms. By the end of this article, readers will better understand how geometric concepts are integrated into machine learning algorithms and how they can be used to improve their performance.</p><p name="b63e" id="b63e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Geometric concepts in machine learning:</strong></p><p name="2982" id="2982" class="graf graf--p graf-after--p">Machine learning algorithms are rooted in mathematical models and rely heavily on geometric concepts to interpret and analyze data. Here, we will overview the key geometric concepts relevant to machine learning algorithms.</p><p name="6acc" id="6acc" class="graf graf--p graf-after--p">One important concept is distance, which measures the difference between two points in space. In machine learning algorithms, distance is often used to measure the similarity between data points, such as in k-nearest neighbor (KNN) algorithms. Similarly, distance is used in hierarchical clustering to group data points into clusters.</p><p name="73e1" id="73e1" class="graf graf--p graf-after--p">Another important geometric concept is an angle, which measures the orientation of objects in space. In machine learning algorithms, angles are important in optimization techniques, such as gradient descent, which adjusts model parameters to minimize the angle between the predicted and actual output.</p><p name="875c" id="875c" class="graf graf--p graf-after--p">Convexity is another critical concept in machine learning algorithms. A convex function has a unique minimum point, and optimization techniques like gradient descent can be used to find this minimum point. Convexity is also relevant to SVMs, where the objective is to find the hyperplane that maximizes the margin between the two classes of data. This objective can be expressed as a convex optimization problem.</p><p name="7d53" id="7d53" class="graf graf--p graf-after--p">Understanding these geometric concepts is essential to understanding how machine learning algorithms work and how they can be optimized for better performance. The next sections will explore specific examples of how geometric concepts are used in various machine learning algorithms.</p><p name="fe94" id="fe94" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Support vector machines (SVMs):</strong></p><p name="7637" id="7637" class="graf graf--p graf-after--p">Support vector machines (SVMs) are a popular machine learning algorithm used for classification tasks. SVMs use a geometric approach to classify data by finding the hyperplane that maximizes the margin between the two classes of data. The margin is the distance between the hyperplane and the closest data points from each class, and SVMs aim to maximize this distance.</p><p name="ae0a" id="ae0a" class="graf graf--p graf-after--p">The hyperplane in SVMs is a geometric concept that separates the two classes of data. It can be represented as a linear equation in high-dimensional space, where each dimension corresponds to a feature of the data. The margin is also a geometric concept, as it measures the distance between the hyperplane and the closest data points from each class.</p><p name="8889" id="8889" class="graf graf--p graf-after--p">SVMs can be implemented and optimized using geometric concepts such as convex optimization. The objective of SVMs is to find the hyperplane that maximizes the margin, which can be formulated as a convex optimization problem. The solution to this problem can be found using optimization techniques such as gradient descent or quadratic programming.</p><p name="046a" id="046a" class="graf graf--p graf-after--p">For example, consider a dataset with two classes of data points, red and blue, that are not linearly separable in 2D space. By mapping these data points to a higher-dimensional space using a kernel function, we can find a hyperplane that separates the two classes. The margin is the distance between the hyperplane and the closest data points from each class, and we can use optimization techniques to find the hyperplane that maximizes this distance.</p><p name="4731" id="4731" class="graf graf--p graf-after--p">Overall, SVMs demonstrate how geometric concepts can be used to classify data and optimize machine learning algorithms. By understanding the geometry behind SVMs, we can gain insights into how they work and how they can be improved for better performance.</p><p name="ff7f" id="ff7f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Codes:</strong></p><p name="c2ae" id="c2ae" class="graf graf--p graf-after--p">This code creates an instance of an SVM classifier with a linear kernel, trains it on the iris dataset, and plots the decision boundary and the support vectors. The decision boundary is the line that separates the two classes of data, and the support vectors are the data points closest to the decision boundary.</p><p name="dda2" id="dda2" class="graf graf--p graf-after--p">The resulting plot shows how the SVM classifier has learned to separate the different classes of data in the iris dataset using a linear decision boundary. You can experiment with different SVM kernels and datasets to see how the decision boundary changes.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="6dc0" id="6dc0" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm, datasets<br /><br /><span class="hljs-comment"># Import iris dataset</span><br />iris = datasets.load_iris()<br /><br /><span class="hljs-comment"># We will only use the first two features of the iris dataset</span><br />X = iris.data[:, :<span class="hljs-number">2</span>]<br />y = iris.target<br /><br /><span class="hljs-comment"># Create an instance of SVM with a linear kernel</span><br />C = <span class="hljs-number">1.0</span>  <span class="hljs-comment"># SVM regularization parameter</span><br />clf = svm.SVC(kernel=<span class="hljs-string">&#x27;linear&#x27;</span>, C=C)<br /><br /><span class="hljs-comment"># Train the SVM on the iris dataset</span><br />clf.fit(X, y)<br /><br /><span class="hljs-comment"># Plot the decision boundary</span><br /><span class="hljs-comment"># Code adapted from: https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html</span><br /><span class="hljs-comment"># Create a meshgrid of points to plot</span><br />x_min, x_max = X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br />y_min, y_max = X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br />xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="hljs-number">0.02</span>),<br />                     np.arange(y_min, y_max, <span class="hljs-number">0.02</span>))<br /><br /><span class="hljs-comment"># Plot the decision boundary and the support vectors</span><br />Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])<br />Z = Z.reshape(xx.shape)<br />plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=<span class="hljs-number">0.8</span>)<br />plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y, cmap=plt.cm.Paired)<br />plt.xlabel(<span class="hljs-string">&#x27;Sepal length&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;Sepal width&#x27;</span>)<br />plt.xlim(xx.<span class="hljs-built_in">min</span>(), xx.<span class="hljs-built_in">max</span>())<br />plt.ylim(yy.<span class="hljs-built_in">min</span>(), yy.<span class="hljs-built_in">max</span>())<br />plt.xticks(())<br />plt.yticks(())<br />plt.title(<span class="hljs-string">&#x27;SVM Decision Boundary&#x27;</span>)<br />plt.show()</span></pre><figure name="2157" id="2157" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*aCU0zy0rnmaSz5hzi-Kuuw.png" data-width="540" data-height="434" src="https://cdn-images-1.medium.com/max/800/1*aCU0zy0rnmaSz5hzi-Kuuw.png"></figure><p name="7121" id="7121" class="graf graf--p graf-after--figure">This code creates an SVM classifier with a linear kernel and varying values of the regularization parameter <code class="markup--code markup--p-code">C</code>. It then trains the SVM on the iris dataset and plots the decision boundary and support vectors for each value of <code class="markup--code markup--p-code">C</code>. The resulting plot shows how increasing the value of <code class="markup--code markup--p-code">C</code> leads to a narrower margin and more support vectors, while decreasing the value of <code class="markup--code markup--p-code">C</code> leads to a wider margin and fewer support vectors.</p><p name="d2ba" id="d2ba" class="graf graf--p graf-after--p">By visualizing the effect of <code class="markup--code markup--p-code">C</code> on the decision boundary of an SVM, we can gain a better understanding of how the regularization parameter affects the performance of the classifier.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="cbb4" id="cbb4" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm, datasets<br /><br /><span class="hljs-comment"># Import iris dataset</span><br />iris = datasets.load_iris()<br /><br /><span class="hljs-comment"># We will only use the first two features of the iris dataset</span><br />X = iris.data[:, :<span class="hljs-number">2</span>]<br />y = iris.target<br /><br /><span class="hljs-comment"># Create a range of values for the regularization parameter C</span><br />C_range = np.logspace(-<span class="hljs-number">2</span>, <span class="hljs-number">10</span>, <span class="hljs-number">13</span>)<br /><br /><span class="hljs-comment"># Plot the decision boundary for different values of C</span><br />plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">8</span>))<br /><span class="hljs-keyword">for</span> i, C <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(C_range):<br />    clf = svm.SVC(kernel=<span class="hljs-string">&#x27;linear&#x27;</span>, C=C)<br />    clf.fit(X, y)<br />    plt.subplot(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, i + <span class="hljs-number">1</span>)<br /><br />    <span class="hljs-comment"># Create a meshgrid of points to plot</span><br />    x_min, x_max = X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br />    y_min, y_max = X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span><br />    xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="hljs-number">0.02</span>),<br />                         np.arange(y_min, y_max, <span class="hljs-number">0.02</span>))<br /><br />    <span class="hljs-comment"># Plot the decision boundary and the support vectors</span><br />    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])<br />    Z = Z.reshape(xx.shape)<br />    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=<span class="hljs-number">0.8</span>)<br />    plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y, cmap=plt.cm.Paired)<br />    plt.xlabel(<span class="hljs-string">&#x27;Sepal length&#x27;</span>)<br />    plt.ylabel(<span class="hljs-string">&#x27;Sepal width&#x27;</span>)<br />    plt.xlim(xx.<span class="hljs-built_in">min</span>(), xx.<span class="hljs-built_in">max</span>())<br />    plt.ylim(yy.<span class="hljs-built_in">min</span>(), yy.<span class="hljs-built_in">max</span>())<br />    plt.xticks(())<br />    plt.yticks(())<br />    plt.title(<span class="hljs-string">&#x27;C = %0.2f&#x27;</span> % C)<br /><br />plt.suptitle(<span class="hljs-string">&#x27;Effect of regularization parameter C on SVM decision boundary&#x27;</span>)<br />plt.tight_layout()<br />plt.show()</span></pre><figure name="dcea" id="dcea" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*MErqu-yv3Ccn9Resw2c0ew.png" data-width="1189" data-height="789" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*MErqu-yv3Ccn9Resw2c0ew.png"></figure><p name="c22e" id="c22e" class="graf graf--p graf-after--figure">This code creates an SVM classifier with a radial basis function (RBF) kernel and trains it on a non-linearly separable dataset by adding noise to the iris dataset. It then plots the decision boundary and support vectors of the SVM. Since the RBF kernel is a non-linear kernel, the resulting decision boundary is also non-linear, and can capture more complex patterns in the data.</p><p name="e8a8" id="e8a8" class="graf graf--p graf-after--p">By visualizing the decision boundary of a non-linear SVM using a kernel trick, we can see how SVMs can be used to solve non-linear classification problems.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="f8d6" id="f8d6" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm, datasets<br /><br /><span class="hljs-comment"># Import iris dataset</span><br />iris = datasets.load_iris()<br /><br /><span class="hljs-comment"># We will only use the first two features of the iris dataset</span><br />X = iris.data[:, :<span class="hljs-number">2</span>]<br />y = iris.target<br /><br /><span class="hljs-comment"># Create a non-linearly separable dataset by adding noise</span><br />np.random.seed(<span class="hljs-number">0</span>)<br />X = np.concatenate((X, np.random.randn(<span class="hljs-number">100</span>, <span class="hljs-number">2</span>) * <span class="hljs-number">0.5</span> + np.array([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])))<br />y = np.concatenate((y, np.array([<span class="hljs-number">1</span>] * <span class="hljs-number">100</span>)))<br /><br /><span class="hljs-comment"># Create an SVM classifier with a radial basis function (RBF) kernel</span><br />clf = svm.SVC(kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>, gamma=<span class="hljs-string">&#x27;auto&#x27;</span>)<br />clf.fit(X, y)<br /><br /><span class="hljs-comment"># Plot the decision boundary and the support vectors</span><br />plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br />plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y, cmap=plt.cm.Paired)<br />plt.xlabel(<span class="hljs-string">&#x27;Sepal length&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;Sepal width&#x27;</span>)<br />plt.xlim(X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">0.5</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">0.5</span>)<br />plt.ylim(X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">0.5</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">0.5</span>)<br /><br /><span class="hljs-comment"># Create a meshgrid of points to plot</span><br />xx, yy = np.meshgrid(np.linspace(X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">0.5</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">0.5</span>, <span class="hljs-number">500</span>),<br />                     np.linspace(X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">0.5</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">0.5</span>, <span class="hljs-number">500</span>))<br /><br /><span class="hljs-comment"># Predict the class for each point in the meshgrid and plot the decision boundary</span><br />Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])<br />Z = Z.reshape(xx.shape)<br />plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=<span class="hljs-number">0.8</span>)<br /><br /><span class="hljs-comment"># Plot the support vectors</span><br />sv = clf.support_vectors_<br />plt.scatter(sv[:, <span class="hljs-number">0</span>], sv[:, <span class="hljs-number">1</span>], s=<span class="hljs-number">100</span>, facecolors=<span class="hljs-string">&#x27;none&#x27;</span>, edgecolors=<span class="hljs-string">&#x27;k&#x27;</span>)<br /><br />plt.title(<span class="hljs-string">&#x27;Non-linear SVM decision boundary using RBF kernel&#x27;</span>)<br />plt.show()</span></pre><figure name="4c37" id="4c37" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*aGBY2WdAL1HyfhBgx_uzcw.png" data-width="833" data-height="699" src="https://cdn-images-1.medium.com/max/800/1*aGBY2WdAL1HyfhBgx_uzcw.png"></figure><p name="503b" id="503b" class="graf graf--p graf-after--figure">This code creates an SVM classifier with a linear kernel and trains it on a linearly separable subset of the iris dataset. It then plots the decision boundary and support vectors of the SVM. Since the linear kernel results in a linear decision boundary, the SVM tries to find the hyperplane that separates the two classes with the maximum margin.</p><p name="d4c5" id="d4c5" class="graf graf--p graf-after--p">By visualizing the decision boundary of a linear SVM, we can see how SVMs can be used to solve linear classification problems by finding the hyperplane that maximizes the margin between the classes.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="f8bb" id="f8bb" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm, datasets<br /><br /><span class="hljs-comment"># Import iris dataset</span><br />iris = datasets.load_iris()<br /><br /><span class="hljs-comment"># We will only use the first two features of the iris dataset</span><br />X = iris.data[:, :<span class="hljs-number">2</span>]<br />y = iris.target<br /><br /><span class="hljs-comment"># Create a linearly separable dataset by removing some data points</span><br />X = X[y != <span class="hljs-number">0</span>]<br />y = y[y != <span class="hljs-number">0</span>]<br />y -= <span class="hljs-number">1</span><br /><br /><span class="hljs-comment"># Create an SVM classifier with a linear kernel</span><br />clf = svm.SVC(kernel=<span class="hljs-string">&#x27;linear&#x27;</span>)<br />clf.fit(X, y)<br /><br /><span class="hljs-comment"># Plot the decision boundary and the support vectors</span><br />plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br />plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y, cmap=plt.cm.Paired)<br />plt.xlabel(<span class="hljs-string">&#x27;Sepal length&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;Sepal width&#x27;</span>)<br />plt.xlim(X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">0.5</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">0.5</span>)<br />plt.ylim(X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">0.5</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">0.5</span>)<br /><br /><span class="hljs-comment"># Get the coefficients and intercept of the hyperplane</span><br />w = clf.coef_[<span class="hljs-number">0</span>]<br />a = -w[<span class="hljs-number">0</span>] / w[<span class="hljs-number">1</span>]<br />xx = np.linspace(X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">0.5</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">0.5</span>)<br />yy = a * xx - (clf.intercept_[<span class="hljs-number">0</span>]) / w[<span class="hljs-number">1</span>]<br /><br /><span class="hljs-comment"># Plot the hyperplane</span><br />plt.plot(xx, yy, <span class="hljs-string">&#x27;k-&#x27;</span>)<br /><br /><span class="hljs-comment"># Plot the margin</span><br />margin = <span class="hljs-number">1</span> / np.sqrt(np.<span class="hljs-built_in">sum</span>(clf.coef_ ** <span class="hljs-number">2</span>))<br />yy_down = yy - a * margin<br />yy_up = yy + a * margin<br />plt.plot(xx, yy_down, <span class="hljs-string">&#x27;k--&#x27;</span>)<br />plt.plot(xx, yy_up, <span class="hljs-string">&#x27;k--&#x27;</span>)<br /><br /><span class="hljs-comment"># Plot the support vectors</span><br />sv = clf.support_vectors_<br />plt.scatter(sv[:, <span class="hljs-number">0</span>], sv[:, <span class="hljs-number">1</span>], s=<span class="hljs-number">100</span>, facecolors=<span class="hljs-string">&#x27;none&#x27;</span>, edgecolors=<span class="hljs-string">&#x27;k&#x27;</span>)<br /><br />plt.title(<span class="hljs-string">&#x27;Linear SVM decision boundary&#x27;</span>)<br />plt.show()</span></pre><figure name="30f0" id="30f0" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*d81_ETNmmJsEKF_B6dDu-A.png" data-width="846" data-height="699" src="https://cdn-images-1.medium.com/max/800/1*d81_ETNmmJsEKF_B6dDu-A.png"></figure><p name="1d13" id="1d13" class="graf graf--p graf-after--figure">This code creates an SVM classifier with a radial basis function (RBF) kernel and trains it on a non-linearly separable subset of the iris dataset. It then creates a mesh grid to plot the decision surface of the SVM, which shows how the SVM classifies points in the feature space based on their distance from the support vectors.</p><p name="7c6f" id="7c6f" class="graf graf--p graf-after--p">By visualizing the decision boundary of a non-linear SVM, we can see how SVMs can be used to solve non-linear classification problems by mapping the input data into a higher-dimensional feature space, where it is more likely to be linearly separable.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="e9c2" id="e9c2" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm, datasets<br /><br /><span class="hljs-comment"># Import iris dataset</span><br />iris = datasets.load_iris()<br /><br /><span class="hljs-comment"># We will only use the first two features of the iris dataset</span><br />X = iris.data[:, :<span class="hljs-number">2</span>]<br />y = iris.target<br /><br /><span class="hljs-comment"># Create a non-linearly separable dataset by adding some noise</span><br />np.random.seed(<span class="hljs-number">0</span>)<br />X = np.vstack((X[y == <span class="hljs-number">0</span>][:<span class="hljs-number">50</span>] + np.random.randn(<span class="hljs-number">50</span>, <span class="hljs-number">2</span>),<br />               X[y == <span class="hljs-number">1</span>][:<span class="hljs-number">50</span>] + np.random.randn(<span class="hljs-number">50</span>, <span class="hljs-number">2</span>),<br />               X[y == <span class="hljs-number">2</span>][:<span class="hljs-number">50</span>] + np.random.randn(<span class="hljs-number">50</span>, <span class="hljs-number">2</span>)))<br />y = np.hstack((np.zeros(<span class="hljs-number">50</span>), np.ones(<span class="hljs-number">50</span>), np.ones(<span class="hljs-number">50</span>) * <span class="hljs-number">2</span>))<br /><br /><span class="hljs-comment"># Create an SVM classifier with a radial basis function (RBF) kernel</span><br />clf = svm.SVC(kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>, gamma=<span class="hljs-number">0.7</span>, C=<span class="hljs-number">1.0</span>)<br />clf.fit(X, y)<br /><br /><span class="hljs-comment"># Create a meshgrid to plot the decision surface</span><br />xx, yy = np.meshgrid(np.linspace(X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">0.5</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">0.5</span>, <span class="hljs-number">500</span>),<br />                     np.linspace(X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">0.5</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">0.5</span>, <span class="hljs-number">500</span>))<br />Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])<br />Z = Z.reshape(xx.shape)<br /><br /><span class="hljs-comment"># Plot the decision surface</span><br />plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br />plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=<span class="hljs-number">0.8</span>)<br />plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y, cmap=plt.cm.Paired)<br /><br />plt.xlabel(<span class="hljs-string">&#x27;Sepal length&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;Sepal width&#x27;</span>)<br />plt.xlim(xx.<span class="hljs-built_in">min</span>(), xx.<span class="hljs-built_in">max</span>())<br />plt.ylim(yy.<span class="hljs-built_in">min</span>(), yy.<span class="hljs-built_in">max</span>())<br /><br />plt.title(<span class="hljs-string">&#x27;Non-linear SVM decision boundary&#x27;</span>)<br />plt.show()</span></pre><figure name="9905" id="9905" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*ySdUuIVtT39k75sOpxv5Mg.png" data-width="833" data-height="699" src="https://cdn-images-1.medium.com/max/800/1*ySdUuIVtT39k75sOpxv5Mg.png"></figure><p name="7926" id="7926" class="graf graf--p graf-after--figure">This code creates a binary SVM classifier with a linear kernel and trains it on a subset of the breast cancer dataset. It then computes the predicted probabilities for the test set and uses them to compute the false positive and true positive rates for various threshold values. It finally plots the ROC curve for the classifier and computes the area under the curve (AUC).</p><p name="62ef" id="62ef" class="graf graf--p graf-after--p">By visualizing a binary SVM classifier&#39;s ROC curve and AUC score, we can evaluate the classifier&#39;s performance and compare it to other classifiers using the same evaluation metric.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="260c" id="260c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm, datasets<br /><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_curve, auc<br /><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br /><br /><span class="hljs-comment"># Import breast cancer dataset</span><br />cancer = datasets.load_breast_cancer()<br /><br /><span class="hljs-comment"># We will only use the first two features of the breast cancer dataset</span><br />X = cancer.data[:, :<span class="hljs-number">2</span>]<br />y = cancer.target<br /><br /><span class="hljs-comment"># Split the data into training and testing sets</span><br />X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)<br /><br /><span class="hljs-comment"># Create a linear SVM classifier</span><br />clf = svm.SVC(kernel=<span class="hljs-string">&#x27;linear&#x27;</span>, probability=<span class="hljs-literal">True</span>)<br />clf.fit(X_train, y_train)<br /><br /><span class="hljs-comment"># Compute the predicted probabilities for the test set</span><br />y_prob = clf.predict_proba(X_test)[:, <span class="hljs-number">1</span>]<br /><br /><span class="hljs-comment"># Compute the false positive rate and true positive rate for various threshold values</span><br />fpr, tpr, thresholds = roc_curve(y_test, y_prob)<br /><br /><span class="hljs-comment"># Compute the area under the ROC curve (AUC)</span><br />roc_auc = auc(fpr, tpr)<br /><br /><span class="hljs-comment"># Plot the ROC curve</span><br />plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br />plt.plot(fpr, tpr, color=<span class="hljs-string">&#x27;darkorange&#x27;</span>, lw=<span class="hljs-number">2</span>, label=<span class="hljs-string">&#x27;ROC curve (AUC = %0.2f)&#x27;</span> % roc_auc)<br />plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], color=<span class="hljs-string">&#x27;navy&#x27;</span>, lw=<span class="hljs-number">2</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br />plt.xlim([<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>])<br />plt.ylim([<span class="hljs-number">0.0</span>, <span class="hljs-number">1.05</span>])<br />plt.xlabel(<span class="hljs-string">&#x27;False Positive Rate&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;True Positive Rate&#x27;</span>)<br />plt.title(<span class="hljs-string">&#x27;Receiver Operating Characteristic (ROC) curve&#x27;</span>)<br />plt.legend(loc=<span class="hljs-string">&quot;lower right&quot;</span>)<br />plt.show()</span></pre><figure name="e653" id="e653" class="graf graf--figure graf-after--pre graf--trailing"><img class="graf-image" data-image-id="1*-pQOWLYELtfFX7bWn5NBIg.png" data-width="857" data-height="699" src="https://cdn-images-1.medium.com/max/800/1*-pQOWLYELtfFX7bWn5NBIg.png"></figure></div></div></section><section name="22ba" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="0523" id="0523" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong">Principal component analysis (PCA)</strong></p><ul class="postList"><li name="280b" id="280b" class="graf graf--li graf-after--p">Explanation of PCA and how it uses geometric concepts to find directions of maximum variance in data:</li><li name="1784" id="1784" class="graf graf--li graf-after--li">Principal component analysis (PCA) is a widely used technique for reducing the dimensionality of high-dimensional data while preserving important information. In this section, we will provide a more detailed explanation of how PCA works and how it uses geometric concepts to find directions of maximum variance in the data.</li><li name="46db" id="46db" class="graf graf--li graf-after--li">PCA starts by finding the directions along which the data varies the most, and these directions are called principal components. The first principal component is the direction that captures the most variance in the data. The second principal component is the direction orthogonal to the first that captures the most variance, and so on.</li><li name="302e" id="302e" class="graf graf--li graf-after--li">Discussion of how eigenvectors and covariance matrix are geometric concepts in PCA:</li><li name="9942" id="9942" class="graf graf--li graf-after--li">This section will explain how eigenvectors and eigenvalues are used to calculate the principal components in PCA. Eigenvectors represent the directions of maximum variance in the data, while eigenvalues represent the magnitude of that variance.</li><li name="c2c7" id="c2c7" class="graf graf--li graf-after--li">We will also explain how the covariance matrix is another important geometric concept in PCA. The covariance matrix is a symmetric matrix representing the pairwise relationships between the features in the data. The diagonal elements of the covariance matrix represent the variances of each feature, while the off-diagonal elements represent the covariances between each pair of features.</li><li name="00ba" id="00ba" class="graf graf--li graf-after--li">Example of PCA implementation and optimization using geometric concepts:</li><li name="73d5" id="73d5" class="graf graf--li graf-after--li">This section will provide a practical example of implementing PCA using Python’s sci-kit-learn library. We will explain how to calculate and visualize the principal components to gain insights into the data structure.</li><li name="e28a" id="e28a" class="graf graf--li graf-after--li">We will also explain how to optimize the number of principal components used in the analysis to balance information preservation and computational complexity. This involves calculating the explained variance ratio for each principal component and selecting the appropriate number of components based on a predetermined threshold or using cross-validation.</li><li name="d3ca" id="d3ca" class="graf graf--li graf-after--li">The section will end with a discussion of how to interpret the results of PCA, including how to map the original features back onto the principal components and visualize the data in the reduced-dimensional space.</li></ul><p name="b360" id="b360" class="graf graf--p graf-after--li">PCA is a statistical method that transforms a set of high-dimensional data points into a lower-dimensional space while preserving as much of the original variation in the data as possible. This is done by finding the directions of maximum variance in the data, called the principal components. The first principal component is the direction that captures the most variance in the data. Each subsequent component captures the maximum remaining variance in the orthogonal direction to the previous components.</p><p name="5954" id="5954" class="graf graf--p graf-after--p">PCA works by calculating the eigenvectors and eigenvalues of the covariance matrix of the data. The eigenvectors represent the directions of maximum variance in the data, and the corresponding eigenvalues represent the magnitude of that variance. We can identify the principal components that capture the most variation in the data by finding the eigenvectors with the largest eigenvalues.</p><p name="49d3" id="49d3" class="graf graf--p graf-after--p">The geometric concept behind PCA is that the data points can be seen as vectors in a high-dimensional space. The principal components are the directions along which the data points vary the most, forming an orthonormal basis for the space. The magnitude of the eigenvalues associated with each principal component indicates how much of the total variation in the data is captured by that component.</p><p name="d8dd" id="d8dd" class="graf graf--p graf-after--p">PCA is a powerful tool for analyzing high-dimensional data and identifying the most important patterns and trends. Reducing the dimensionality of the data, it can also help to improve the efficiency and interpretability of subsequent analyses.</p><p name="96f5" id="96f5" class="graf graf--p graf-after--p">Let us first define what they are to understand how eigenvectors and eigenvalues are used in PCA.</p><p name="53c8" id="53c8" class="graf graf--p graf-after--p">An eigenvector of a matrix A is a non-zero vector v that, when multiplied by A, results in a scalar multiple of v. In other words, Av = λv, where λ is the eigenvalue corresponding to eigenvector v. Eigenvectors and eigenvalues describe how a linear transformation affects a vector.</p><p name="06c6" id="06c6" class="graf graf--p graf-after--p">In PCA, we calculate the eigenvectors and eigenvalues of the covariance matrix of the data. The covariance matrix is a symmetric matrix that describes the pairwise relationships between the features in the data. The diagonal elements of the covariance matrix represent the variances of each feature, while the off-diagonal elements represent the covariances between each pair of features.</p><p name="d043" id="d043" class="graf graf--p graf-after--p">The eigenvectors of the covariance matrix represent the directions of maximum variance in the data. The eigenvector with the largest eigenvalue is the direction of maximum variance or the first principal component. The second principal component is the eigenvector with the second largest eigenvalue, and so on.</p><p name="0303" id="0303" class="graf graf--p graf-after--p">The eigenvalues themselves represent the amount of variance explained by each principal component. The sum of all the eigenvalues is equal to the total variance of the data. We can use this information to determine how many principal components to retain in the reduced-dimensional representation of the data.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="e3c9" id="e3c9" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><br /><span class="hljs-comment"># generate 1000 random 2-dimensional points</span><br />points = np.random.randn(<span class="hljs-number">2</span>, <span class="hljs-number">1000</span>)<br /><br /><span class="hljs-comment"># compute the covariance matrix</span><br />covariance_matrix = np.cov(points)<br /><br /><span class="hljs-comment"># compute the eigenvalues and eigenvectors</span><br />eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)<br /><br /><span class="hljs-comment"># plot the points</span><br />plt.scatter(points[<span class="hljs-number">0</span>], points[<span class="hljs-number">1</span>], alpha=<span class="hljs-number">0.2</span>)<br /><br /><span class="hljs-comment"># plot the eigenvectors</span><br />plt.plot([<span class="hljs-number">0</span>, eigenvectors[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], [<span class="hljs-number">0</span>, eigenvectors[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]], <span class="hljs-string">&#x27;r&#x27;</span>, label=<span class="hljs-string">&#x27;Eigenvector 1&#x27;</span>)<br />plt.plot([<span class="hljs-number">0</span>, eigenvectors[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]], [<span class="hljs-number">0</span>, eigenvectors[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Eigenvector 2&#x27;</span>)<br /><br /><span class="hljs-comment"># plot the eigenvalues</span><br />plt.scatter(eigenvalues[<span class="hljs-number">0</span>]*eigenvectors[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], eigenvalues[<span class="hljs-number">0</span>]*eigenvectors[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], s=<span class="hljs-number">100</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;Eigenvalue 1&#x27;</span>)<br />plt.scatter(eigenvalues[<span class="hljs-number">1</span>]*eigenvectors[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], eigenvalues[<span class="hljs-number">1</span>]*eigenvectors[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], s=<span class="hljs-number">100</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">&#x27;Eigenvalue 2&#x27;</span>)<br /><br /><span class="hljs-comment"># set axis limits and labels</span><br />plt.xlim(-<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br />plt.ylim(-<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br />plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br />plt.legend()<br /><br /><span class="hljs-comment"># show the plot</span><br />plt.show()</span></pre><figure name="e07a" id="e07a" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*vXzDn93fugUhbBM3qtvxDg.png" data-width="570" data-height="438" src="https://cdn-images-1.medium.com/max/800/1*vXzDn93fugUhbBM3qtvxDg.png"></figure><p name="84cf" id="84cf" class="graf graf--p graf-after--figure">The covariance matrix is an important geometric concept in PCA because it represents the pairwise relationships between the features in the data. By calculating the eigenvectors and eigenvalues of the covariance matrix, we can identify the directions of maximum variance in the data and the amount of variance explained by each principal component. This allows us to reduce the dimensionality of the data while retaining as much information as possible.</p><p name="5133" id="5133" class="graf graf--p graf-after--p">This code generates a scatter plot of the original data using <code class="markup--code markup--p-code">matplotlib.pyplot</code>. The <code class="markup--code markup--p-code">np.random.seed</code> function is used to ensure the reproducibility of the data. The scatter plot is then created using <code class="markup--code markup--p-code">ax.scatter</code>, with the <code class="markup--code markup--p-code">xlabel</code>, <code class="markup--code markup--p-code">ylabel</code>, and <code class="markup--code markup--p-code">title</code> set using <code class="markup--code markup--p-code">ax.set_xlabel</code>, <code class="markup--code markup--p-code">ax.set_ylabel</code>, and <code class="markup--code markup--p-code">ax.set_title</code>, respectively. Finally, the plot is displayed using <code class="markup--code markup--p-code">plt.show()</code>.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="b17a" id="b17a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><br /><span class="hljs-comment"># Generate sample data</span><br />np.random.seed(<span class="hljs-number">0</span>)<br />x = np.random.normal(size=<span class="hljs-number">100</span>)<br />y = <span class="hljs-number">2</span> * x + np.random.normal(size=<span class="hljs-number">100</span>)<br /><br /><span class="hljs-comment"># Plot scatter plot</span><br />fig, ax = plt.subplots()<br />ax.scatter(x, y)<br />ax.set_xlabel(<span class="hljs-string">&#x27;X&#x27;</span>)<br />ax.set_ylabel(<span class="hljs-string">&#x27;Y&#x27;</span>)<br />ax.set_title(<span class="hljs-string">&#x27;Scatter Plot of Original Data&#x27;</span>)<br />plt.show()</span></pre><figure name="908d" id="908d" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*OK73KB_jwC4An_aFpSvZ1w.png" data-width="565" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*OK73KB_jwC4An_aFpSvZ1w.png"></figure><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="php" name="316b" id="316b" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> sklearn.decomposition import PCA<br /><br /><span class="hljs-comment"># Fit PCA model to data</span><br />data = np.<span class="hljs-keyword">array</span>([x, y]).T<br />pca = <span class="hljs-title function_ invoke__">PCA</span>(n_components=<span class="hljs-number">2</span>)<br />pca.<span class="hljs-title function_ invoke__">fit</span>(data)<br /><br /><span class="hljs-comment"># Project data onto first principal component</span><br />data_transformed = pca.<span class="hljs-title function_ invoke__">transform</span>(data)<br />first_pc = pca.components_[<span class="hljs-number">0</span>]<br /><br /><span class="hljs-comment"># Plot scatter plot of data projected onto first principal component</span><br />fig, ax = plt.<span class="hljs-title function_ invoke__">subplots</span>()<br />ax.<span class="hljs-title function_ invoke__">scatter</span>(data_transformed[:, <span class="hljs-number">0</span>], np.<span class="hljs-title function_ invoke__">zeros_like</span>(data_transformed[:, <span class="hljs-number">0</span>]))<br />ax.<span class="hljs-title function_ invoke__">arrow</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, first_pc[<span class="hljs-number">0</span>], first_pc[<span class="hljs-number">1</span>], head_width=<span class="hljs-number">0.1</span>, head_length=<span class="hljs-number">0.1</span>, fc=<span class="hljs-string">&#x27;k&#x27;</span>, ec=<span class="hljs-string">&#x27;k&#x27;</span>)<br />ax.<span class="hljs-title function_ invoke__">set_xlim</span>([-<span class="hljs-number">4</span>, <span class="hljs-number">4</span>])<br />ax.<span class="hljs-title function_ invoke__">set_xlabel</span>(<span class="hljs-string">&#x27;First Principal Component&#x27;</span>)<br />ax.<span class="hljs-title function_ invoke__">set_title</span>(<span class="hljs-string">&#x27;Data Projected onto First Principal Component&#x27;</span>)<br />plt.<span class="hljs-title function_ invoke__">show</span>()</span></pre><figure name="91d1" id="91d1" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*3wJDQznxSdX3-UXkW16p4w.png" data-width="563" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*3wJDQznxSdX3-UXkW16p4w.png"></figure><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="314d" id="314d" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"><span class="hljs-comment"># Generate sample data</span><br />np.random.seed(<span class="hljs-number">0</span>)<br />data = np.random.normal(size=(<span class="hljs-number">100</span>, <span class="hljs-number">4</span>))<br /><br /><span class="hljs-comment"># Fit PCA model to data</span><br />pca = PCA(n_components=<span class="hljs-number">4</span>)<br />pca.fit(data)<br /><br /><span class="hljs-comment"># Plot scree plot of PCA model</span><br />fig, ax = plt.subplots()<br />ax.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>), pca.explained_variance_ratio_, <span class="hljs-string">&#x27;o-&#x27;</span>)<br />ax.set_xlabel(<span class="hljs-string">&#x27;Principal Component&#x27;</span>)<br />ax.set_ylabel(<span class="hljs-string">&#x27;Explained Variance Ratio&#x27;</span>)<br />ax.set_title(<span class="hljs-string">&#x27;Scree Plot&#x27;</span>)<br />plt.show()</span></pre><figure name="09e0" id="09e0" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*46rIXqCI7vj38drrR2chKQ.png" data-width="576" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*46rIXqCI7vj38drrR2chKQ.png"></figure><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="kotlin" name="3151" id="3151" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"># Generate sample <span class="hljs-keyword">data</span><br />np.random.seed(<span class="hljs-number">0</span>)<br /><span class="hljs-keyword">data</span> = np.random.normal(size=(<span class="hljs-number">100</span>, <span class="hljs-number">4</span>))<br /><br /># Fit PCA model to <span class="hljs-keyword">data</span><br />pca = PCA(n_components=<span class="hljs-number">2</span>)<br />pca.fit(<span class="hljs-keyword">data</span>)<br /><br /># Plot biplot of PCA model<br />fig, ax = plt.subplots()<br />ax.scatter(<span class="hljs-keyword">data</span>[:, <span class="hljs-number">0</span>], <span class="hljs-keyword">data</span>[:, <span class="hljs-number">1</span>])<br /><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(pca.components_.shape[<span class="hljs-number">1</span>]):<br />    ax.arrow(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, pca.components_[<span class="hljs-number">0</span>, i], pca.components_[<span class="hljs-number">1</span>, i], head_width=<span class="hljs-number">0.1</span>, head_length=<span class="hljs-number">0.1</span>, fc=<span class="hljs-string">&#x27;k&#x27;</span>, ec=<span class="hljs-string">&#x27;k&#x27;</span>)<br />    ax.text(pca.components_[<span class="hljs-number">0</span>, i], pca.components_[<span class="hljs-number">1</span>, i], f<span class="hljs-string">&#x27;Feature {i+1}&#x27;</span>, ha=<span class="hljs-string">&#x27;center&#x27;</span>, va=<span class="hljs-string">&#x27;center&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br />ax.set_xlabel(<span class="hljs-string">&#x27;Feature 1&#x27;</span>)<br />ax.set_ylabel(<span class="hljs-string">&#x27;Feature 2&#x27;</span>)<br />ax.set_title(<span class="hljs-string">&#x27;Biplot&#x27;</span>)<br />plt.show()</span></pre><figure name="0535" id="0535" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*QuqnmKSVNa8Oh4ggZ7I_qg.png" data-width="579" data-height="453" src="https://cdn-images-1.medium.com/max/800/1*QuqnmKSVNa8Oh4ggZ7I_qg.png"></figure><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="1acf" id="1acf" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"><span class="hljs-comment"># Reconstruct data from first two principal components</span><br />data_reconstructed = pca.inverse_transform(data_pca)<br /><br /><span class="hljs-comment"># Plot reconstructed data</span><br />fig = plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br />ax = fig.add_subplot(<span class="hljs-number">111</span>, projection=<span class="hljs-string">&#x27;3d&#x27;</span>)<br />ax.scatter(data_reconstructed[:, <span class="hljs-number">0</span>], data_reconstructed[:, <span class="hljs-number">1</span>], data_reconstructed[:, <span class="hljs-number">2</span>])<br />ax.set_xlabel(<span class="hljs-string">&#x27;X&#x27;</span>)<br />ax.set_ylabel(<span class="hljs-string">&#x27;Y&#x27;</span>)<br />ax.set_zlabel(<span class="hljs-string">&#x27;Z&#x27;</span>)</span></pre><figure name="e870" id="e870" class="graf graf--figure graf-after--pre graf--trailing"><img class="graf-image" data-image-id="1*7-BTff3RSE40h8OWT-utJA.png" data-width="663" data-height="636" src="https://cdn-images-1.medium.com/max/800/1*7-BTff3RSE40h8OWT-utJA.png"></figure></div></div></section><section name="1197" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="94ff" id="94ff" class="graf graf--p graf--leading">In this article, we have discussed the importance of understanding the geometry of machine learning algorithms. We started by defining what geometry means in the context of machine learning and how it relates to the optimization process of these algorithms. We then explored different geometric concepts, such as convexity, curvature, and distance, and how they can affect the performance and behavior of machine learning models.</p><p name="4767" id="4767" class="graf graf--p graf-after--p">We also discussed using visualization techniques to gain insights into the geometry of high-dimensional spaces, which can be particularly challenging to understand. By visualizing machine learning models&#39; decision boundaries and feature spaces, we can better understand how they make predictions and identify potential issues or biases.</p><p name="ae2c" id="ae2c" class="graf graf--p graf-after--p">Understanding the geometry of machine learning algorithms can be particularly useful in improving their performance. For example, by understanding the curvature of the loss function, we can choose appropriate optimization methods that are more suitable for the problem at hand. Similarly, by visualizing the feature space, we can identify regions where the model may be overfitting or underfitting the data.</p><p name="7559" id="7559" class="graf graf--p graf-after--p graf--trailing">In conclusion, the geometry of machine learning algorithms is an important area of study that can help us improve the performance and interpretability of these models. By gaining a deeper understanding of the geometry of high-dimensional spaces, we can develop more effective machine-learning techniques and better understand the behavior of these algorithms.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@the_daft_introvert" class="p-author h-card">Vishal Sharma</a> on <a href="https://medium.com/p/7f69ee3892c9"><time class="dt-published" datetime="2023-03-07T17:40:23.984Z">March 7, 2023</time></a>.</p><p><a href="https://medium.com/@the_daft_introvert/beyond-the-numbers-how-understanding-the-geometry-of-machine-learning-can-boost-performance-7f69ee3892c9" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 2, 2023.</p></footer></article></body></html>