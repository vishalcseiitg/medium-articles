<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>“Expectation: The Hidden Key to Unlocking the Mysteries of Probability and Decision-Making”</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">“Expectation: The Hidden Key to Unlocking the Mysteries of Probability and Decision-Making”</h1>
</header>
<section data-field="subtitle" class="p-summary">
Introduction:
</section>
<section data-field="body" class="e-content">
<section name="c6db" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4349" id="4349" class="graf graf--h3 graf--startsWithDoubleQuote graf--leading graf--title">“Expectation: The Hidden Key to Unlocking the Mysteries of Probability and Decision-Making”</h3><figure name="e6d1" id="e6d1" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*oshdh2E4WaMGKppW" data-width="3032" data-height="2021" data-unsplash-photo-id="taZldX9vnwk" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*oshdh2E4WaMGKppW"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@jannerboy62?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@jannerboy62?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Nick Fewings</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure><h4 name="a28b" id="a28b" class="graf graf--h4 graf-after--figure">Introduction:</h4><p name="75f8" id="75f8" class="graf graf--p graf-after--h4">Probability theory is a branch of mathematics that deals with the study of random events and the likelihood of their occurrence. It provides a framework for understanding and analyzing uncertain events and has applications in a wide range of fields, including finance, engineering, physics, computer science, and more.</p><p name="73b2" id="73b2" class="graf graf--p graf-after--p">One of the key concepts in probability theory is the idea of expectation, which refers to the average or expected value of a random variable. The expectation of a random variable is a numerical value that represents the long-term average outcome of repeated trials of an experiment or observation.</p><p name="1b17" id="1b17" class="graf graf--p graf-after--p">The expectation of a random variable is the weighted average of all possible outcomes, where the weights are given by their corresponding probabilities. It provides a way to summarize the central tendency of a distribution of random variables and is used in various statistical analyses, decision-making processes, and risk assessments.</p><h4 name="9c06" id="9c06" class="graf graf--h4 graf-after--p">Definition of Expectation:</h4><p name="4947" id="4947" class="graf graf--p graf-after--h4">The expectation of a random variable is a numerical value that represents the average value of the variable over a large number of trials or observations. Mathematically, the expectation of a random variable X is denoted by E(X) or μ and is defined as:</p><blockquote name="953c" id="953c" class="graf graf--blockquote graf-after--p">E(X) = Σx P(X = x)</blockquote><p name="5eda" id="5eda" class="graf graf--p graf-after--blockquote">where Σx denotes the sum over all possible values of X, and P(X = x) denotes the probability that X takes the value x.</p><p name="aa65" id="aa65" class="graf graf--p graf-after--p">In other words, the expectation of a random variable is the sum of the products of each possible value of the variable and its corresponding probability, weighted by the probabilities of each value occurring.</p><p name="3141" id="3141" class="graf graf--p graf-after--p">Let us look at some examples of calculating the expectation of random variables, both for discrete and continuous cases:</p><p name="6b7f" id="6b7f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 1:</strong></p><p name="ebb0" id="ebb0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Discrete Random Variable:</strong> Suppose we roll a fair six-sided die. Let X be the random variable representing the value of the die roll. Then X can take values from 1 to 6 with equal probability, so the probability mass function (PMF) of X is:</p><p name="0d3e" id="0d3e" class="graf graf--p graf-after--p">P(X = x) = 1/6 for x = 1, 2, 3, 4, 5, 6</p><p name="38bd" id="38bd" class="graf graf--p graf-after--p">The expectation of X is, therefore:</p><blockquote name="6fbc" id="6fbc" class="graf graf--blockquote graf-after--p">E(X) = Σx P(X = x) = (1/6) * (1 + 2 + 3 + 4 + 5 + 6) = 3.5</blockquote><p name="1d71" id="1d71" class="graf graf--p graf-after--blockquote">So the expected value of the die roll is 3.5.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="php" name="404b" id="404b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Import necessary libraries</span><br />import numpy <span class="hljs-keyword">as</span> np<br /><br /><span class="hljs-comment"># Define a probability distribution</span><br />X = np.<span class="hljs-keyword">array</span>([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<br />p = np.<span class="hljs-keyword">array</span>([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.2</span>])<br /><br /><span class="hljs-comment"># Calculate the expectation of X</span><br />expectation = np.<span class="hljs-title function_ invoke__">sum</span>(X * p)<br /><br /><span class="hljs-comment"># Print the result</span><br /><span class="hljs-keyword">print</span>(<span class="hljs-string">&quot;The expectation of X is:&quot;</span>, expectation)</span></pre><p name="93ad" id="93ad" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Example 2:</strong></p><p name="8d54" id="8d54" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Continuous Random Variable:</strong> Suppose we have a continuous random variable Y with a probability density function (PDF) given by:</p><blockquote name="164d" id="164d" class="graf graf--blockquote graf-after--p">f(y) = 3y for 0 ≤ y ≤ 1</blockquote><p name="9a2a" id="9a2a" class="graf graf--p graf-after--blockquote">To calculate the expectation of Y, we need to integrate the product of y and its PDF over the range of possible values of Y, which is [0,1]:</p><blockquote name="cd93" id="cd93" class="graf graf--blockquote graf-after--p">E(Y) = ∫ y f(y) dy = ∫ y (3y) dy from 0 to 1 = [y³] from 0 to 1 = 1/3</blockquote><p name="baf6" id="baf6" class="graf graf--p graf-after--blockquote">So the expected value of Y is 1/3.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="539e" id="539e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Import necessary libraries</span><br /><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">from</span> scipy.integrate <span class="hljs-keyword">import</span> quad<br /><br /><span class="hljs-comment"># Define the probability density function (pdf)</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">pdf</span>(<span class="hljs-params">x</span>):<br />    <span class="hljs-keyword">return</span> np.exp(-x)  <span class="hljs-comment"># Exponential distribution with lambda = 1</span><br /><br /><span class="hljs-comment"># Define the range of integration</span><br />a, b = <span class="hljs-number">0</span>, np.inf  <span class="hljs-comment"># Integration range from 0 to infinity</span><br /><br /><span class="hljs-comment"># Calculate the expectation of X</span><br />expectation, _ = quad(<span class="hljs-keyword">lambda</span> x: x * pdf(x), a, b)<br /><br /><span class="hljs-comment"># Print the result</span><br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The expectation of X is:&quot;</span>, expectation)</span></pre><p name="bd7a" id="bd7a" class="graf graf--p graf-after--pre">The expectation of a random variable is a measure of its central tendency and represents the average value of the variable over repeated trials or observations. It can be calculated using the mathematical formula E(X) = Σx P(X = x) for discrete random variables and E(X) = ∫ y f(y) dy for continuous random variables.</p><p name="f944" id="f944" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Properties of Expectation:</strong></p><p name="8bb0" id="8bb0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Linearity</strong>: The expectation operator satisfies linearity, which means that it distributes over linear combinations of random variables. In other words, for any constants a and b and any two random variables X and Y, we have:</p><blockquote name="53cc" id="53cc" class="graf graf--blockquote graf-after--p">E(aX + bY) = aE(X) + bE(Y)</blockquote><p name="1e19" id="1e19" class="graf graf--p graf-after--blockquote">This property is useful in many statistical applications, such as regression analysis, where the expected value of a linear combination of variables is a key parameter of interest.</p><p name="9e38" id="9e38" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Monotonicity</strong>: Monotonicity refers to the property in which larger random variable values imply larger expected values. More specifically, if X and Y are two random variables with X ≤ Y (i.e., X is always less than or equal to Y), then we have:</p><blockquote name="d933" id="d933" class="graf graf--blockquote graf-after--p">E(X) ≤ E(Y)</blockquote><p name="58d5" id="58d5" class="graf graf--p graf-after--blockquote">This property follows from the definition of expectation, as larger values of Y contribute more to the expected value than smaller values of X.</p><p name="194f" id="194f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Additivity</strong>: Additivity refers to the property that the expected value of the sum of two random variables is the sum of their expected values. More specifically, if X and Y are two random variables, then we have:</p><blockquote name="b8aa" id="b8aa" class="graf graf--blockquote graf-after--p">E(X + Y) = E(X) + E(Y)</blockquote><p name="67de" id="67de" class="graf graf--p graf-after--blockquote">This property is useful in many statistical applications, such as calculating the expected value of the difference between two random variables or analyzing the effect of adding or removing random variables from a system.</p><p name="e3ae" id="e3ae" class="graf graf--p graf-after--p">To see why this property holds, consider the definition of expectation:</p><blockquote name="e63b" id="e63b" class="graf graf--blockquote graf-after--p">E(X + Y) = Σ(x+y)P(X=x, Y=y)</blockquote><p name="98de" id="98de" class="graf graf--p graf-after--blockquote">Expanding the sum using the distributive property, we get:</p><p name="5b10" id="5b10" class="graf graf--p graf-after--p">E(X + Y) = ΣxΣy(x+y)P(X=x, Y=y)</p><p name="56df" id="56df" class="graf graf--p graf-after--p">= ΣxΣy(xP(X=x, Y=y) + yP(X=x, Y=y))</p><p name="d911" id="d911" class="graf graf--p graf-after--p">= ΣxΣy(xP(X=x, Y=y)) + ΣxΣy(yP(X=x, Y=y))</p><p name="d669" id="d669" class="graf graf--p graf-after--p">= Σx(xΣyP(X=x, Y=y)) + Σy(yΣxP(X=x, Y=y))</p><p name="4698" id="4698" class="graf graf--p graf-after--p">= Σx(xP(X=x) + yP(X=x)) + Σy(xP(Y=y) + yP(Y=y))</p><p name="dca7" id="dca7" class="graf graf--p graf-after--p">= Σx(xP(X=x)) + Σx(yP(X=x)) + Σy(xP(Y=y)) + Σy(yP(Y=y))</p><p name="422c" id="422c" class="graf graf--p graf-after--p">= E(X) + E(Y)</p><p name="5acf" id="5acf" class="graf graf--p graf-after--p">Hence, the expected value of the sum of two random variables is equal to the sum of their expected values.</p><p name="c0fe" id="c0fe" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Expectation and Probability Distributions:</strong></p><p name="2b4d" id="2b4d" class="graf graf--p graf-after--p">The expectation of a random variable is a summary measure that characterizes the center of its probability distribution. Specifically, it represents the average value the random variable takes, weighted by the probabilities of each possible outcome.</p><p name="1448" id="1448" class="graf graf--p graf-after--p">Mathematically, the expectation of a random variable X, denoted as E(X), is defined as:</p><blockquote name="52b0" id="52b0" class="graf graf--blockquote graf-after--p">E(X) = ∑ x P(X=x)</blockquote><p name="db2a" id="db2a" class="graf graf--p graf-after--blockquote">where x represents each possible outcome of X, and P(X=x) represents the probability of X taking the value x.</p><p name="0d0b" id="0d0b" class="graf graf--p graf-after--p">The relationship between the expectation and the probability distribution of a random variable is that the expectation can be used to describe the shape and location of the distribution. For example, if the expected value of a random variable is higher than its median, the distribution is said to be skewed to the right. If the expected value is equal to the median, the distribution is said to be symmetric. If the expected value is less than the median, the distribution is said to be skewed to the left.</p><p name="6bc1" id="6bc1" class="graf graf--p graf-after--p">Here are some examples of probability distributions and their corresponding expectations:</p><ol class="postList"><li name="25a0" id="25a0" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Bernoulli distribution:</strong> This is a discrete probability distribution with two possible outcomes (success or failure) and a single parameter p, which represents the probability of success. The expected value of a Bernoulli random variable X is given by:</li></ol><blockquote name="1d5e" id="1d5e" class="graf graf--blockquote graf-after--li">E(X) = p(1) + (1-p)(0) = p</blockquote><ol class="postList"><li name="45e9" id="45e9" class="graf graf--li graf-after--blockquote"><strong class="markup--strong markup--li-strong">Normal distribution: </strong>This continuous probability distribution is often used to model real-world phenomena. The expected value of a normal random variable X with mean μ and variance σ² is given by:</li></ol><blockquote name="fb4e" id="fb4e" class="graf graf--blockquote graf-after--li">E(X) = μ</blockquote><p name="cb95" id="cb95" class="graf graf--p graf-after--blockquote">The expected value of a normal distribution is equal to its mean.</p><ol class="postList"><li name="0fea" id="0fea" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Poisson distribution: </strong>This is a discrete probability distribution often used to model the number of occurrences of a certain event in a given time interval. The expected value of a Poisson random variable X with parameter λ is given by:</li></ol><blockquote name="eaba" id="eaba" class="graf graf--blockquote graf-after--li">E(X) = λ</blockquote><p name="8379" id="8379" class="graf graf--p graf-after--blockquote">The expected value of a Poisson distribution is equal to its parameter λ, which represents the average rate of occurrence.</p><p name="5e53" id="5e53" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Conditional Expectation:</strong></p><p name="59ef" id="59ef" class="graf graf--p graf-after--p">The conditional expectation is a way to calculate the expected value of a random variable X, given some information about another random variable Y. In other words, the conditional expectation of X given Y denoted as E(X|Y), is X&#39;s expected value when we know Y&#39;s value.</p><p name="49da" id="49da" class="graf graf--p graf-after--p">Mathematically, the conditional expectation of X given Y is defined as:</p><blockquote name="df5f" id="df5f" class="graf graf--blockquote graf-after--p">E(X|Y) = ∑ x P(X=x|Y)</blockquote><p name="c16a" id="c16a" class="graf graf--p graf-after--blockquote">where x represents each possible outcome of X, and P(X=x|Y) represents the probability of X taking the value x, given that Y has taken some value.</p><p name="fb28" id="fb28" class="graf graf--p graf-after--p">The law of total expectation, also known as the law of iterated expectation, states that the expected value of a random variable X can be calculated as the weighted average of its conditional expectations with respect to different values of another random variable Y. Mathematically, this can be written as:</p><blockquote name="c312" id="c312" class="graf graf--blockquote graf-after--p">E(X) = E(E(X|Y))</blockquote><p name="f404" id="f404" class="graf graf--p graf-after--blockquote">where the inner expectation E(X|Y) is taken with respect to the conditional distribution of X, given Y.</p><p name="2389" id="2389" class="graf graf--p graf-after--p">The law of iterated expectation is a special case of the law of total expectation, which states that the expected value of a conditional expectation is equal to the original expectation. Mathematically, this can be written as:</p><blockquote name="f8c7" id="f8c7" class="graf graf--blockquote graf-after--p">E(E(X|Y)) = E(X)</blockquote><p name="9d3b" id="9d3b" class="graf graf--p graf-after--blockquote">Here are some examples of calculating conditional expectation:</p><ol class="postList"><li name="015d" id="015d" class="graf graf--li graf-after--p">Let X and Y be two discrete random variables with the following joint probability distribution:</li></ol><blockquote name="7cff" id="7cff" class="graf graf--blockquote graf-after--li">P(X=x, Y=y) = { 1/6, if x+y=2; 1/3, if x+y=3; 1/2, if x+y=4; 0, otherwise }</blockquote><blockquote name="8fe3" id="8fe3" class="graf graf--blockquote graf-after--blockquote">Find E(X|Y=2).</blockquote><p name="e208" id="e208" class="graf graf--p graf-after--blockquote">To find E(X|Y=2), we need to calculate the conditional distribution of X, given Y=2:</p><p name="a4df" id="a4df" class="graf graf--p graf-after--p">P(X=x|Y=2)</p><p name="f66e" id="f66e" class="graf graf--p graf-after--p">= P(X=x, Y=2) / P(Y=2)</p><p name="cd1b" id="cd1b" class="graf graf--p graf-after--p">= P(X=x, Y=2) / [P(X=1, Y=2) + P(X=2, Y=2)]</p><p name="1882" id="1882" class="graf graf--p graf-after--p">Plugging in the values from the joint distribution, we get:</p><p name="44c0" id="44c0" class="graf graf--p graf-after--p">P(X=1|Y=2)</p><p name="e0b8" id="e0b8" class="graf graf--p graf-after--p">= 1/3, and P(X=2|Y=2)</p><p name="4f9b" id="4f9b" class="graf graf--p graf-after--p">= 2/3</p><p name="4e7f" id="4e7f" class="graf graf--p graf-after--p">Therefore, the conditional expectation of X given Y=2 is:</p><p name="7d06" id="7d06" class="graf graf--p graf-after--p">E(X|Y=2) = (1/3)(1) + (2/3)(2) = 5/3</p><ol class="postList"><li name="0772" id="0772" class="graf graf--li graf-after--p">Let X and Y be two continuous random variables, with the joint probability density function given by:</li></ol><blockquote name="ab36" id="ab36" class="graf graf--blockquote graf-after--li">f(x,y) = { 6xy, if 0 ≤ x ≤ y ≤ 1; 0, otherwise }</blockquote><blockquote name="5c4d" id="5c4d" class="graf graf--blockquote graf-after--blockquote">Find E(Y|X=1/2).</blockquote><p name="d246" id="d246" class="graf graf--p graf-after--blockquote">To find E(Y|X=1/2), we need to calculate the conditional distribution of Y, given X=1/2:</p><p name="dd26" id="dd26" class="graf graf--p graf-after--p">f(y|x=1/2)</p><p name="77bc" id="77bc" class="graf graf--p graf-after--p">= f(x=1/2,y) / ∫ f(x=1/2,y) dy</p><p name="d03f" id="d03f" class="graf graf--p graf-after--p">Plugging in the values from the joint density function, we get:</p><p name="7aba" id="7aba" class="graf graf--p graf-after--p">f(y|x=1/2) = 12y, if 1/2 ≤ y ≤ 1; 0, otherwise</p><p name="3a42" id="3a42" class="graf graf--p graf-after--p">Therefore, the conditional expectation of Y given X=1/2 is:</p><p name="70d7" id="70d7" class="graf graf--p graf-after--p">E(Y|X=1/2)</p><p name="4f83" id="4f83" class="graf graf--p graf-after--p">= ∫ y f(y|x=1/2) dy</p><p name="10de" id="10de" class="graf graf--p graf-after--p">= ∫ 1/2 y f(y|x=1/2) * 2 dy</p><p name="297f" id="297f" class="graf graf--p graf-after--p">= ∫ 1/2 y (12y) * 2 dy, from 1/2 to 1</p><p name="21f6" id="21f6" class="graf graf--p graf-after--p">= 35/48</p><p name="ba6b" id="ba6b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Applications of Expectation:</strong></p><p name="286b" id="286b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Finance:</strong></p><p name="a244" id="a244" class="graf graf--p graf-after--p">The concept of expectation is used extensively in investment analysis, risk management, and pricing of financial derivatives. For example, the expected return is used to estimate the average return that an investment is expected to generate over a given period. The concept of variance, closely related to the expectation, is used to measure the risk associated with an investment. The Black-Scholes formula, widely used to price options, is based on the concept of expectation.</p><p name="0959" id="0959" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Engineering:</strong></p><p name="882d" id="882d" class="graf graf--p graf-after--p">The concept of expectation is used in reliability analysis and optimization. For example, in reliability analysis, the expected value of the time to failure is used to estimate the expected lifetime of a system or component. In optimization, the expected value of the objective function is often used to evaluate a system&#39;s performance and find the optimal design parameters.</p><p name="4907" id="4907" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Physics:</strong></p><p name="69e5" id="69e5" class="graf graf--p graf-after--p">The concept of expectation is used in statistical mechanics, quantum mechanics, and thermodynamics. For example, in statistical mechanics, the expectation value of the energy is used to calculate the thermodynamic properties of a system. In quantum mechanics, the expectation value of an observable is used to calculate the probability of obtaining a particular measurement outcome.</p><p name="abdc" id="abdc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Conclusion:</strong></p><p name="87a0" id="87a0" class="graf graf--p graf-after--p">In this article, we discussed the concept of expectation in probability theory, a mathematical tool used to calculate the average value of a random variable. We explained how the expectation operator satisfies linearity, monotonicity, and additivity and how it can describe a random variable&#39;s probability distribution.</p><p name="7b2a" id="7b2a" class="graf graf--p graf-after--p graf--trailing">We also introduced the concept of conditional expectation and discussed the law of total expectation and the law of iterated expectation. Furthermore, we showed how expectation is used in various fields, such as finance, engineering, and physics.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@the_daft_introvert" class="p-author h-card">Vishal Sharma</a> on <a href="https://medium.com/p/fefec7c2ed5a"><time class="dt-published" datetime="2023-03-15T06:56:43.555Z">March 15, 2023</time></a>.</p><p><a href="https://medium.com/@the_daft_introvert/expectation-the-hidden-key-to-unlocking-the-mysteries-of-probability-and-decision-making-fefec7c2ed5a" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 2, 2023.</p></footer></article></body></html>