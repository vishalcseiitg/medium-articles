<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>“Uncovering Hidden Patterns: Harnessing the Power of KD Trees for High-Dimensional Data Analysis”</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">“Uncovering Hidden Patterns: Harnessing the Power of KD Trees for High-Dimensional Data Analysis”</h1>
</header>
<section data-field="subtitle" class="p-summary">
Kd trees have a long history in computer science and data structures, dating back to the 1970s. They were first proposed by computer…
</section>
<section data-field="body" class="e-content">
<section name="f009" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="c573" id="c573" class="graf graf--h3 graf--startsWithDoubleQuote graf--leading graf--title">“Uncovering Hidden Patterns: Harnessing the Power of KD Trees for High-Dimensional Data Analysis”</h3><figure name="8113" id="8113" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*_cJeEBpAcZMEyNQn" data-width="2850" data-height="4032" data-unsplash-photo-id="8Ec7nkceSWU" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*_cJeEBpAcZMEyNQn"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@devu_62442?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@devu_62442?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Devyani Vij</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure><p name="c108" id="c108" class="graf graf--p graf-after--figure">Kd trees have a long history in computer science and data structures, dating back to the 1970s. They were first proposed by computer scientist Jon Bentley in his 1975 paper “Multidimensional Binary Search Trees Used for Associative Searching,” where he introduced the concept of using binary trees to search for data points in a multi-dimensional space.</p><p name="497f" id="497f" class="graf graf--p graf-after--p">Since then, kd trees have been widely used in many areas of computer science, including machine learning and data science. One of the earliest applications of kd trees in machine learning was in computer vision, where they were used for efficient image matching and recognition. In the 1990s, kd trees were also used in natural language processing for information retrieval and document clustering.</p><p name="e1ee" id="e1ee" class="graf graf--p graf-after--p">Today, kd trees remain an important tool in many areas of machine learning and data science, particularly in high-dimensional spaces where other search algorithms become inefficient. They are commonly used in recommendation systems, anomaly detection, and data mining applications.</p><p name="848a" id="848a" class="graf graf--p graf-after--p">Despite their effectiveness, kd trees do have some limitations. One of the main challenges is dealing with skewed data distributions, where the data points are not evenly distributed across all dimensions. The tree may need to be more balanced, leading to efficient search times. Various modifications to the standard kd tree algorithm have been proposed to address this problem, such as the balanced kd tree and the orthogonal range tree.</p><p name="1796" id="1796" class="graf graf--p graf-after--p">There are several types of kd trees, each with variations on the standard algorithm. Here are some of the most commonly used types:</p><ol class="postList"><li name="0649" id="0649" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Standard kd tree:</strong> This is the most basic version of the kd tree algorithm, in which the data is partitioned based on the median value of a single axis at each level of the tree.</li><li name="685d" id="685d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Balanced kd tree:</strong> In a standard kd tree, the tree can become unbalanced if the data is skewed towards one side of a particular axis. The balanced kd tree algorithm addresses this issue by selecting the splitting axis based on the longest side of the hyperrectangle containing the data rather than the dimension with the highest variance.</li><li name="b0e4" id="b0e4" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Compact kd tree:</strong> In a standard kd tree, each node in the tree requires storing the split axis and split value and the indices of the points in the subset. The compact kd tree algorithm reduces the storage requirements by encoding this information in a single integer value, using the bit representation of the split axis and split value.</li><li name="f57d" id="f57d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Cover tree: </strong>The cover tree algorithm is a variation of the kd tree designed to handle data that is not evenly distributed across all dimensions. It recursively divides the data into subsets with a fixed covering radius around a selected point rather than using a fixed splitting axis and value.</li><li name="8d9a" id="8d9a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Ball tree:</strong> The algorithm is another kd tree variation designed to handle non-uniformly distributed data. It works by partitioning the data into hyperspheres rather than hyperplanes, with each node representing a hypersphere that contains a subset of the data.</li><li name="5014" id="5014" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Range tree:</strong> A range tree is a kd tree specifically designed for answering range queries, where the goal is to find all points within a certain distance of a query point. Range trees store an additional level of data structures at each node of the tree, which allows for efficient range queries without having to search through all the points in the tree.</li></ol><p name="523d" id="523d" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">What is a kd tree?</strong></p><p name="82d8" id="82d8" class="graf graf--p graf-after--p">A kd tree (short for k-dimensional tree) is a data structure used for efficient nearest neighbor search in a multi-dimensional space. It is a binary tree where each node represents a hyperplane that partitions the space into two regions. The hyperplanes are aligned with the coordinate axes, and the splitting is performed along the dimension with the largest data variance.</p><p name="f1a3" id="f1a3" class="graf graf--p graf-after--p">In other words, a kd tree is a way to organize points in a multi-dimensional space so that they can be quickly searched and located based on their proximity to a given query point. The tree is built recursively by partitioning the data into two halves at each tree level based on the value of a chosen coordinate. This results in a binary tree where each leaf node contains a single point in the space, and each internal node contains a hyperplane that splits the space into two subspaces.</p><p name="080d" id="080d" class="graf graf--p graf-after--p">The main advantage of using a kd tree for nearest neighbor search is that it efficiently retrieves points closest to a given query point. Instead of searching through all the points in the space, the search can be confined to a small subset of the data by traversing the tree. This can result in significant speedups, especially in high-dimensional spaces where traditional methods could be more efficient.</p><p name="5d12" id="5d12" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">How does kd tree work?</strong></p><p name="63a2" id="63a2" class="graf graf--p graf-after--p">Kd trees work by recursively partitioning the data into smaller subsets using hyperplanes aligned with the space&#39;s coordinate axes. The hyperplanes divide the space into two regions, each corresponding to a subtree in the tree.</p><p name="801b" id="801b" class="graf graf--p graf-after--p">Constructing a kd tree begins by selecting a splitting axis based on the dimension with the largest variance in the data. The median value of the points along that axis is chosen as the splitting value, and the hyperplane that passes through that point is used to partition the data. The points are divided into two subsets based on which side of the hyperplane they lie on, and each subset is recursively partitioned in the same way until a stopping criterion is met. The stopping criterion could be a maximum depth limit or a minimum number of points in each leaf node.</p><p name="e7cc" id="e7cc" class="graf graf--p graf-after--p">Once the tree is constructed, the nearest neighbor search can be performed by traversing the tree from the root node to a leaf node and keeping track of the closest point encountered along the way. At each node, the distance between the query point and the hyperplane is computed, and the algorithm determines which subtree to search next based on which side of the hyperplane the query point is on. The search continues until a leaf node is reached, at which point the algorithm returns the point in that leaf node that is closest to the query point.</p><p name="a2b0" id="a2b0" class="graf graf--p graf-after--p">Kd trees can also be used for k-nearest neighbor search, where the algorithm returns the k points in the space closest to the query point. This is achieved by maintaining a priority queue of the k closest points encountered during the search and updating it as necessary at each leaf node.</p><p name="1416" id="1416" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Algorithm:</strong></p><ol class="postList"><li name="e3ae" id="e3ae" class="graf graf--li graf-after--p">Choose the splitting axis: The first step is selecting the axis to partition the data. This is typically done by selecting the axis with the highest variance in the data.</li><li name="f04a" id="f04a" class="graf graf--li graf-after--li">Find the median value: Next, the median value along the chosen axis is computed. This is done by sorting the data along the chosen axis and selecting the middle value.</li><li name="b2fe" id="b2fe" class="graf graf--li graf-after--li">Partition of the data: The data is then divided into two subsets based on whether each point is greater or less than the median value along the chosen axis. The left subset contains all points with values less than the median, and the right subset contains all points with values greater than the median.</li><li name="b2bc" id="b2bc" class="graf graf--li graf-after--li">Recursively build the tree: The above steps are repeated for each data subset, with a new splitting axis chosen at each tree level. This process continues until each subset contains only a single point, at which point a leaf node is created.</li><li name="b140" id="b140" class="graf graf--li graf-after--li">Connect the nodes: Once all the leaf nodes have been created, the tree is constructed by connecting the nodes from the bottom up. Each node stores the splitting axis and value and pointers to its left and right children.</li></ol><p name="34fa" id="34fa" class="graf graf--p graf-after--li">During the tree&#39;s construction, several techniques can be used to improve the efficiency and accuracy of the algorithm. These include balancing the tree to ensure it remains evenly distributed, using alternative splitting criteria to handle skewed data distributions, and using pruning techniques to remove redundant nodes and improve search times.</p><p name="f00e" id="f00e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">An Example:</strong></p><p name="e4f8" id="e4f8" class="graf graf--p graf-after--p">Let’s consider the following dataset of points in a 2D space:</p><p name="e461" id="e461" class="graf graf--p graf-after--p">[(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)]</p><p name="864f" id="864f" class="graf graf--p graf-after--p">We follow the algorithm described earlier to construct a kd tree from this dataset. We start by choosing the splitting axis as the x-axis since it has the highest variance in the data. We then find the median value of the x-coordinates, which is 5. We partition the data into two subsets based on whether the x-coordinate is less than or greater than 5:</p><p name="ebed" id="ebed" class="graf graf--p graf-after--p">Left subset: [(2,3), (4,7), (3,2)]</p><p name="faa7" id="faa7" class="graf graf--p graf-after--p">Right subset: [(5,4), (9,6), (8,1)]</p><p name="2aed" id="2aed" class="graf graf--p graf-after--p">We then repeat the above steps for each subset, choosing the y-axis as the splitting axis this time. For the left subset, the median value of the y-coordinates is 3.5, so we partition the data into:</p><p name="b214" id="b214" class="graf graf--p graf-after--p">The left subset of the left subset: [(2,3), (3,2)]</p><p name="6fc0" id="6fc0" class="graf graf--p graf-after--p">The right subset of the left subset: [(4,7)]</p><p name="7714" id="7714" class="graf graf--p graf-after--p">For the right subset, the median value of the y-coordinates is 4, so we partition the data into:</p><p name="e214" id="e214" class="graf graf--p graf-after--p">Left subset of right subset: [(5,4)]</p><p name="c966" id="c966" class="graf graf--p graf-after--p">Right subset of right subset: [(9,6), (8,1)]</p><p name="5fa8" id="5fa8" class="graf graf--p graf-after--p">We continue this process until each subset contains only a single point, at which point we create a leaf node. The resulting kd tree for this dataset is shown below:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="typescript" name="3682" id="3682" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">          (<span class="hljs-number">5</span>,<span class="hljs-number">4</span>)<br />          /   \<br />   (<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)/(<span class="hljs-number">4</span>,<span class="hljs-number">7</span>) (<span class="hljs-number">9</span>,<span class="hljs-number">6</span>)<br />      /       \<br />(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)/(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)   (<span class="hljs-number">8</span>,<span class="hljs-number">1</span>)</span></pre><p name="5815" id="5815" class="graf graf--p graf-after--pre">Note that the splitting axis alternates at each tree level, and the points are partitioned into subsets based on their values along that axis. The resulting tree can be used for efficient nearest-neighbor search and other machine learning and data science applications.</p><p name="9464" id="9464" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Pseudocode:</strong></p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="java" name="a0f1" id="a0f1" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">function <span class="hljs-title function_">build_kd_tree</span><span class="hljs-params">(points, depth=<span class="hljs-number">0</span>)</span><br />    n = length(points)<br />    <span class="hljs-keyword">if</span> n == <span class="hljs-number">0</span>:<br />        <span class="hljs-keyword">return</span> <span class="hljs-type">None</span><br />    <span class="hljs-variable">axis</span> <span class="hljs-operator">=</span> depth % <span class="hljs-type">k</span><br />    <span class="hljs-variable">sorted_points</span> <span class="hljs-operator">=</span> sort(points, key=lambda p: p[axis])<br />    mid = n <span class="hljs-comment">// 2</span><br />    node = Node(<br />        point=sorted_points[mid],<br />        axis=axis,<br />        left=build_kd_tree(sorted_points[:mid], depth+<span class="hljs-number">1</span>),<br />        right=build_kd_tree(sorted_points[mid+<span class="hljs-number">1</span>:], depth+<span class="hljs-number">1</span>)<br />    )<br />    <span class="hljs-keyword">return</span> node</span></pre><p name="3159" id="3159" class="graf graf--p graf-after--pre">This function takes a set of <code class="markup--code markup--p-code">points</code>, where each point is a k-dimensional vector, and recursively constructs a kd tree. The <code class="markup--code markup--p-code">depth</code> argument keeps track of the tree&#39;s current depth and determines which axis to split on.</p><p name="bc1b" id="bc1b" class="graf graf--p graf-after--p">The base case is when there are no points left to split, in which case the function returns <code class="markup--code markup--p-code">None</code>. Otherwise, the function sorts the points along the current axis and chooses the median point as the subtree&#39;s root. The function then calls itself recursively to build the left and right subtrees from the points to the left and right of the median point, respectively.</p><p name="35b2" id="35b2" class="graf graf--p graf-after--p">The <code class="markup--code markup--p-code">Node</code> class represents each node in the kd tree and has the following attributes:</p><ul class="postList"><li name="2487" id="2487" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">point</code>: the k-dimensional vector representing the point stored at this node</li><li name="b56b" id="b56b" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">axis</code>: the index of the axis used to split the points at this node</li><li name="4c2b" id="4c2b" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">left</code>: the left child node in the tree</li><li name="c068" id="c068" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">right</code>: the right child node in the tree</li></ul><p name="6aa8" id="6aa8" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Python Implementation of kd trees:</strong></p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="2577" id="2577" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><br /><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br />    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, point=<span class="hljs-literal">None</span>, axis=<span class="hljs-literal">None</span>, left=<span class="hljs-literal">None</span>, right=<span class="hljs-literal">None</span></span>):<br />        self.point = point<br />        self.axis = axis<br />        self.left = left<br />        self.right = right<br /><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_kd_tree</span>(<span class="hljs-params">points, depth=<span class="hljs-number">0</span></span>):<br />    n = <span class="hljs-built_in">len</span>(points)<br />    <span class="hljs-keyword">if</span> n == <span class="hljs-number">0</span>:<br />        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br />    axis = depth % <span class="hljs-built_in">len</span>(points[<span class="hljs-number">0</span>])<br />    sorted_points = <span class="hljs-built_in">sorted</span>(points, key=<span class="hljs-keyword">lambda</span> point: point[axis])<br />    mid = n // <span class="hljs-number">2</span><br />    <span class="hljs-keyword">return</span> Node(<br />        point=sorted_points[mid],<br />        axis=axis,<br />        left=build_kd_tree(sorted_points[:mid], depth+<span class="hljs-number">1</span>),<br />        right=build_kd_tree(sorted_points[mid+<span class="hljs-number">1</span>:], depth+<span class="hljs-number">1</span>)<br />    )<br /><br />points = [(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>), (<span class="hljs-number">5</span>,<span class="hljs-number">4</span>), (<span class="hljs-number">9</span>,<span class="hljs-number">6</span>), (<span class="hljs-number">4</span>,<span class="hljs-number">7</span>), (<span class="hljs-number">8</span>,<span class="hljs-number">1</span>), (<span class="hljs-number">7</span>,<span class="hljs-number">2</span>)]<br />root = build_kd_tree(points)</span></pre><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="caf6" id="caf6" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><br /><br />np.random.seed(<span class="hljs-number">42</span>)  <span class="hljs-comment"># for reproducibility</span><br />points = np.random.rand(<span class="hljs-number">100</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># 100 random points in 2D space</span><br /><br />root = build_kd_tree(points)<br /><br /><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_kd_tree</span>(<span class="hljs-params">node, xmin, xmax, ymin, ymax</span>):<br />    <span class="hljs-keyword">if</span> node <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br />        <span class="hljs-keyword">return</span><br />    <span class="hljs-comment"># plot the current node&#x27;s point</span><br />    plt.plot(node.point[<span class="hljs-number">0</span>], node.point[<span class="hljs-number">1</span>], <span class="hljs-string">&#x27;ko&#x27;</span>)<br />    <span class="hljs-keyword">if</span> node.axis == <span class="hljs-number">0</span>:<br />        <span class="hljs-comment"># vertical splitting plane</span><br />        plt.plot([node.point[<span class="hljs-number">0</span>], node.point[<span class="hljs-number">0</span>]], [ymin, ymax], <span class="hljs-string">&#x27;k--&#x27;</span>)<br />        plot_kd_tree(node.left, xmin, node.point[<span class="hljs-number">0</span>], ymin, ymax)<br />        plot_kd_tree(node.right, node.point[<span class="hljs-number">0</span>], xmax, ymin, ymax)<br />    <span class="hljs-keyword">else</span>:<br />        <span class="hljs-comment"># horizontal splitting plane</span><br />        plt.plot([xmin, xmax], [node.point[<span class="hljs-number">1</span>], node.point[<span class="hljs-number">1</span>]], <span class="hljs-string">&#x27;k--&#x27;</span>)<br />        plot_kd_tree(node.left, xmin, xmax, ymin, node.point[<span class="hljs-number">1</span>])<br />        plot_kd_tree(node.right, xmin, xmax, node.point[<span class="hljs-number">1</span>], ymax)<br /><br />plt.figure(figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">6</span>))<br />plt.xlim(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br />plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br />plot_kd_tree(root, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br />plt.show()</span></pre><figure name="ac44" id="ac44" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*sdFspwXY2--P360T81Z78w.png" data-width="527" data-height="511" src="https://cdn-images-1.medium.com/max/800/1*sdFspwXY2--P360T81Z78w.png"></figure><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="5996" id="5996" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><br /><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br />    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, point, axis, left=<span class="hljs-literal">None</span>, right=<span class="hljs-literal">None</span></span>):<br />        self.point = point<br />        self.axis = axis<br />        self.left = left<br />        self.right = right<br /><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_kd_tree</span>(<span class="hljs-params">points, depth=<span class="hljs-number">0</span></span>):<br />    n = <span class="hljs-built_in">len</span>(points)<br />    <span class="hljs-keyword">if</span> n == <span class="hljs-number">0</span>:<br />        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br />    axis = depth % <span class="hljs-number">2</span><br />    sorted_points = <span class="hljs-built_in">sorted</span>(points, key=<span class="hljs-keyword">lambda</span> x: x[axis])<br />    mid = n // <span class="hljs-number">2</span><br />    <span class="hljs-keyword">return</span> Node(<br />        point=sorted_points[mid],<br />        axis=axis,<br />        left=build_kd_tree(sorted_points[:mid], depth + <span class="hljs-number">1</span>),<br />        right=build_kd_tree(sorted_points[mid+<span class="hljs-number">1</span>:], depth + <span class="hljs-number">1</span>)<br />    )<br /><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_kd_tree</span>(<span class="hljs-params">node, xmin, xmax, ymin, ymax, color=<span class="hljs-string">&#x27;r&#x27;</span></span>):<br />    <span class="hljs-keyword">if</span> node <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br />        <span class="hljs-keyword">return</span><br />    <span class="hljs-keyword">if</span> node.axis == <span class="hljs-number">0</span>:<br />        plt.plot([node.point[<span class="hljs-number">0</span>], node.point[<span class="hljs-number">0</span>]], [ymin, ymax], <span class="hljs-string">&#x27;--&#x27;</span>, color=color, linewidth=<span class="hljs-number">1</span>)<br />        plot_kd_tree(node.left, xmin, node.point[<span class="hljs-number">0</span>], ymin, ymax, color=color)<br />        plot_kd_tree(node.right, node.point[<span class="hljs-number">0</span>], xmax, ymin, ymax, color=color)<br />    <span class="hljs-keyword">else</span>:<br />        plt.plot([xmin, xmax], [node.point[<span class="hljs-number">1</span>], node.point[<span class="hljs-number">1</span>]], <span class="hljs-string">&#x27;--&#x27;</span>, color=color, linewidth=<span class="hljs-number">1</span>)<br />        plot_kd_tree(node.left, xmin, xmax, ymin, node.point[<span class="hljs-number">1</span>], color=color)<br />        plot_kd_tree(node.right, xmin, xmax, node.point[<span class="hljs-number">1</span>], ymax, color=color)<br /><br />np.random.seed(<span class="hljs-number">42</span>)<br />points = np.random.rand(<span class="hljs-number">100</span>, <span class="hljs-number">2</span>)<br /><br />plt.figure(figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">6</span>))<br />plt.xlim(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br />plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br />root = build_kd_tree(points)<br />plt.plot(root.point[<span class="hljs-number">0</span>], root.point[<span class="hljs-number">1</span>], <span class="hljs-string">&#x27;bo&#x27;</span>)<br />plot_kd_tree(root, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, color=<span class="hljs-string">&#x27;r&#x27;</span>)<br />plt.scatter(points[:, <span class="hljs-number">0</span>], points[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;k&#x27;</span>, s=<span class="hljs-number">20</span>, alpha=<span class="hljs-number">0.5</span>)<br />plt.show()</span></pre><figure name="d88b" id="d88b" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*F4cj19nstEUQbO2OJioAPQ.png" data-width="527" data-height="511" src="https://cdn-images-1.medium.com/max/800/1*F4cj19nstEUQbO2OJioAPQ.png"></figure><p name="bec2" id="bec2" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Conclusion:</strong></p><p name="39fd" id="39fd" class="graf graf--p graf-after--p">KD trees are a powerful data structure for efficiently searching and querying high-dimensional data. With their ability to partition data points into hierarchical subspaces, KD trees can dramatically reduce search times compared to naive linear search algorithms. KD trees have many applications, including computer graphics, image processing, and machine learning. However, it’s important to note that KD trees have limitations, such as the difficulty of constructing balanced trees and the sensitivity to the insertion order. Nonetheless, with careful implementation and proper use, KD trees can provide significant performance gains and facilitate high-dimensional data processing. As such, KD trees are valuable for data analysis and computational science.</p><p name="69d8" id="69d8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">References:</strong></p><ol class="postList"><li name="bfe8" id="bfe8" class="graf graf--li graf-after--p">Friedman, J., Bentley, J. L., &amp; Finkel, R. A. (1977). An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software (TOMS), 3(3), 209–226. <a href="https://dl.acm.org/doi/10.1145/355744.355745" data-href="https://dl.acm.org/doi/10.1145/355744.355745" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">https://dl.acm.org/doi/10.1145/355744.355745</a></li><li name="7341" id="7341" class="graf graf--li graf-after--li">Bentley, J. L. (1975). Multidimensional binary search trees used for associative searching. Communications of the ACM, 18(9), 509–517. <a href="https://dl.acm.org/doi/10.1145/361002.361007" data-href="https://dl.acm.org/doi/10.1145/361002.361007" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">https://dl.acm.org/doi/10.1145/361002.361007</a></li><li name="463d" id="463d" class="graf graf--li graf-after--li">Dasgupta, A. (2008). Algorithms. McGraw-Hill Higher Education.</li><li name="3fec" id="3fec" class="graf graf--li graf-after--li graf--trailing">Peters, J., De Baets, B., &amp; Vanthienen, J. (2009). A decision tree-based similarity measure for multi-version ontology matching. In International Conference on Knowledge-Based and Intelligent Information and Engineering Systems (pp. 132–139). Springer. <a href="https://link.springer.com/chapter/10.1007/978-3-642-04274-4_16" data-href="https://link.springer.com/chapter/10.1007/978-3-642-04274-4_16" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">https://link.springer.com/chapter/10.1007/978-3-642-04274-4_16</a></li></ol></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@the_daft_introvert" class="p-author h-card">Vishal Sharma</a> on <a href="https://medium.com/p/b01e2068d9fa"><time class="dt-published" datetime="2023-03-09T19:50:59.423Z">March 9, 2023</time></a>.</p><p><a href="https://medium.com/@the_daft_introvert/uncovering-hidden-patterns-harnessing-the-power-of-kd-trees-for-high-dimensional-data-analysis-b01e2068d9fa" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 2, 2023.</p></footer></article></body></html>