<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Utilizing Bayesian Inference: An Approach to Making Predictions through Probability Distributions</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Utilizing Bayesian Inference: An Approach to Making Predictions through Probability Distributions</h1>
</header>
<section data-field="subtitle" class="p-summary">
This article discusses the concept of Bayesian inference, a probabilistic approach to machine learning that involves updating a prior…
</section>
<section data-field="body" class="e-content">
<section name="c10e" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1a76" id="1a76" class="graf graf--h3 graf--leading graf--title">Utilizing Bayesian Inference: An Approach to Making Predictions through Probability Distributions</h3><blockquote name="aa14" id="aa14" class="graf graf--pullquote graf-after--h3">This article discusses the concept of Bayesian inference, a probabilistic approach to machine learning that involves updating a prior probability distribution based on observed data. It explores the use of Bayesian inference in making predictions about future data based on available evidence.</blockquote><figure name="1e7a" id="1e7a" class="graf graf--figure graf-after--pullquote"><img class="graf-image" data-image-id="0*5D4DKWNlEuUuVgke" data-width="7360" data-height="4912" data-unsplash-photo-id="-kwHrztmKzg" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*5D4DKWNlEuUuVgke"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@cafera13?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@cafera13?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Carlos Felipe Ramírez Mesa</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure><p name="7eb9" id="7eb9" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--figure"><span class="graf-dropCap">B</span>ayesian inference is a powerful approach to machine learning that allows us to make predictions and decisions based on incomplete or uncertain information. At its core, Bayesian inference involves updating a prior probability distribution based on new data, which allows us to obtain a posterior probability distribution that reflects the probability of different outcomes given the available evidence.</p><p name="c1e0" id="c1e0" class="graf graf--p graf-after--p">To illustrate how Bayesian inference works in practice, let’s consider a simple example. Suppose we are interested in predicting a customer&#39;s likelihood of buying a certain product based on their demographic information and past purchasing history. We could approach this problem using Bayesian inference as follows:</p><ol class="postList"><li name="444c" id="444c" class="graf graf--li graf-after--p">Define a prior probability distribution: We start by defining a prior probability distribution that reflects our beliefs about the probability of a customer buying the product before we observe any data. This prior distribution can be based on expert knowledge, historical data, or other sources of information.</li><li name="2125" id="2125" class="graf graf--li graf-after--li">Collect data: We then collect data on a set of customers, including their demographic information and past purchasing history, as well as whether or not they bought the product.</li><li name="f69d" id="f69d" class="graf graf--li graf-after--li">Update the prior distribution: Using Bayes’ theorem, we update the prior probability distribution based on the observed data to obtain a posterior probability distribution that reflects the probability of a customer buying the product given their demographic information and past purchasing history.</li><li name="62eb" id="62eb" class="graf graf--li graf-after--li">Make predictions: Finally, we can use the posterior distribution to make predictions about future customers, such as estimating the probability that a new customer will buy the product based on their demographic information and past purchasing history.</li></ol><blockquote name="b3c2" id="b3c2" class="graf graf--pullquote graf-after--li">How maths work in all these?</blockquote><p name="6068" id="6068" class="graf graf--p graf-after--pullquote">Suppose we observed data y and wish to estimate a parameter θ. We start with a prior probability distribution p(θ) that represents our initial belief about the value of θ. Then, after observing the data, we update our belief about θ by computing the posterior distribution <strong class="markup--strong markup--p-strong">p(θ|y) </strong>using Bayes’ theorem:</p><blockquote name="8c3e" id="8c3e" class="graf graf--blockquote graf-after--p">p(θ|y) = p(y|θ) * p(θ) / p(y)</blockquote><p name="7ebe" id="7ebe" class="graf graf--p graf-after--blockquote">where <strong class="markup--strong markup--p-strong">p(y|θ)</strong> is the likelihood function, which measures the probability of observing the data y given the parameter θ, and p(y) is the marginal likelihood or evidence, which is the probability of observing the data y over all possible values of θ. The posterior distribution <strong class="markup--strong markup--p-strong">p(θ|y)</strong> represents the updated belief about the value of θ given the observed data y.</p><p name="03de" id="03de" class="graf graf--p graf-after--p">The posterior distribution can be used to make predictions about future data by computing the posterior predictive distribution:</p><blockquote name="41cf" id="41cf" class="graf graf--blockquote graf-after--p">p(y_new|y) = ∫ p(y_new|θ) * p(θ|y) dθ</blockquote><p name="a571" id="a571" class="graf graf--p graf-after--blockquote">where <strong class="markup--strong markup--p-strong">p(y_new|θ)</strong> is the likelihood of the new data given the parameter θ, and the integral is taken over all possible values of θ. The posterior predictive distribution represents the probability distribution of the new data given the observed data.</p><blockquote name="f116" id="f116" class="graf graf--pullquote graf-after--p">Posterior Distribution:</blockquote><p name="55b7" id="55b7" class="graf graf--p graf-after--pullquote">The posterior distribution represents the updated probability distribution of the parameter of interest. An example of a posterior distribution for a simple model with a single parameter:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="5aaf" id="5aaf" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> pymc3 <span class="hljs-keyword">as</span> pm<br /><span class="hljs-keyword">import</span> arviz <span class="hljs-keyword">as</span> az<br /><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><br /><span class="hljs-comment"># Define the model</span><br /><span class="hljs-keyword">with</span> pm.Model() <span class="hljs-keyword">as</span> model:<br />    <span class="hljs-comment"># Prior distribution</span><br />    theta = pm.Beta(<span class="hljs-string">&#x27;theta&#x27;</span>, alpha=<span class="hljs-number">1</span>, beta=<span class="hljs-number">1</span>)<br />    <span class="hljs-comment"># Observed data</span><br />    y = pm.Bernoulli(<span class="hljs-string">&#x27;y&#x27;</span>, p=theta, observed=[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br /><br />    <span class="hljs-comment"># Compute the posterior distribution</span><br />    trace = pm.sample(<span class="hljs-number">1000</span>, tune=<span class="hljs-number">1000</span>)<br />    posterior = trace[<span class="hljs-string">&#x27;theta&#x27;</span>]<br /><br /><span class="hljs-comment"># Plot the posterior distribution</span><br />az.plot_posterior(posterior)<br />plt.axvline(np.mean(posterior), color=<span class="hljs-string">&#x27;red&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br />plt.title(<span class="hljs-string">&#x27;Posterior Distribution of Theta&#x27;</span>, fontsize=<span class="hljs-number">14</span>)<br />plt.xlabel(<span class="hljs-string">&#x27;Theta&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;Density&#x27;</span>, fontsize=<span class="hljs-number">12</span>)<br />plt.show()</span></pre><figure name="6166" id="6166" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*DF8meq6q-pXJgS1vXVlvoA.png" data-width="548" data-height="464" src="https://cdn-images-1.medium.com/max/800/1*DF8meq6q-pXJgS1vXVlvoA.png"></figure><blockquote name="36b1" id="36b1" class="graf graf--pullquote graf-after--figure">Trace Plot:</blockquote><p name="3199" id="3199" class="graf graf--p graf-after--pullquote">The trace plot shows the samples of the parameter of interest from the Markov chain Monte Carlo (MCMC) algorithm used to estimate the posterior distribution. An example of a trace plot for a simple model with a single parameter:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="1e0a" id="1e0a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">import</span> pymc3 <span class="hljs-keyword">as</span> pm<br /><br /><span class="hljs-comment"># Define a simple model with a single parameter</span><br /><span class="hljs-keyword">with</span> pm.Model() <span class="hljs-keyword">as</span> model:<br />    mu = pm.Normal(<span class="hljs-string">&#x27;mu&#x27;</span>, mu=<span class="hljs-number">0</span>, sd=<span class="hljs-number">1</span>)<br />    y_obs = pm.Normal(<span class="hljs-string">&#x27;y_obs&#x27;</span>, mu=mu, sd=<span class="hljs-number">1</span>, observed=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br /><br /><span class="hljs-comment"># Use MCMC to estimate the posterior distribution</span><br /><span class="hljs-keyword">with</span> model:<br />    trace = pm.sample(<span class="hljs-number">1000</span>)<br /><br /><span class="hljs-comment"># Plot the trace plot</span><br />pm.traceplot(trace, var_names=[<span class="hljs-string">&#x27;mu&#x27;</span>], compact=<span class="hljs-literal">True</span>)<br />plt.xlabel(<span class="hljs-string">&#x27;Parameter Value&#x27;</span>, fontsize=<span class="hljs-number">10</span>)<br />plt.ylabel(<span class="hljs-string">&#x27;Posterior Distribution&#x27;</span>, fontsize=<span class="hljs-number">10</span>)<br />plt.title(<span class="hljs-string">&#x27;Trace Plot of Posterior Distribution&#x27;</span>)<br />plt.show()</span></pre><figure name="4cee" id="4cee" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*e7JaEYz2HoFovGoHtjmXEg.png" data-width="950" data-height="239" src="https://cdn-images-1.medium.com/max/800/1*e7JaEYz2HoFovGoHtjmXEg.png"></figure><blockquote name="ad95" id="ad95" class="graf graf--pullquote graf-after--figure">An Example:</blockquote><p name="9038" id="9038" class="graf graf--p graf-after--pullquote">We simulate a dataset of 1000 customers, where each customer has age and income and a binary purchase history indicating whether they have purchased a product in the past. We define a Bayesian model that assumes a logistic regression relationship between the customer’s age, income, and the probability of purchasing a product. We use normal distributions as priors for the model parameters and sample the posterior distributions using Markov Chain Monte Carlo (MCMC) methods.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="4c7e" id="4c7e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> pymc3 <span class="hljs-keyword">as</span> pm<br /><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><br /><span class="hljs-comment"># simulate customer data</span><br />np.random.seed(<span class="hljs-number">123</span>)<br />N = <span class="hljs-number">1000</span>  <span class="hljs-comment"># number of customers</span><br />age = np.random.normal(<span class="hljs-number">35</span>, <span class="hljs-number">10</span>, N)  <span class="hljs-comment"># customer ages</span><br />income = np.random.normal(<span class="hljs-number">50000</span>, <span class="hljs-number">10000</span>, N)  <span class="hljs-comment"># customer incomes</span><br />purchased = np.zeros(N, dtype=<span class="hljs-built_in">int</span>)  <span class="hljs-comment"># customer purchase histories</span><br />purchased[(age &gt; <span class="hljs-number">30</span>) &amp; (age &lt; <span class="hljs-number">40</span>) &amp; (income &gt; <span class="hljs-number">40000</span>)] = np.random.binomial(<span class="hljs-number">1</span>, <span class="hljs-number">0.5</span>, np.<span class="hljs-built_in">sum</span>((age &gt; <span class="hljs-number">30</span>) &amp; (age &lt; <span class="hljs-number">40</span>) &amp; (income &gt; <span class="hljs-number">40000</span>)))<br /><br /><span class="hljs-comment"># define Bayesian model</span><br /><span class="hljs-keyword">with</span> pm.Model() <span class="hljs-keyword">as</span> model:<br />    <span class="hljs-comment"># prior distributions</span><br />    alpha = pm.Normal(<span class="hljs-string">&#x27;alpha&#x27;</span>, mu=<span class="hljs-number">0</span>, sd=<span class="hljs-number">10</span>)<br />    beta_age = pm.Normal(<span class="hljs-string">&#x27;beta_age&#x27;</span>, mu=<span class="hljs-number">0</span>, sd=<span class="hljs-number">10</span>)<br />    beta_income = pm.Normal(<span class="hljs-string">&#x27;beta_income&#x27;</span>, mu=<span class="hljs-number">0</span>, sd=<span class="hljs-number">10</span>)<br /><br />    <span class="hljs-comment"># likelihood function</span><br />    p = pm.math.sigmoid(alpha + beta_age*age + beta_income*income)<br />    y = pm.Bernoulli(<span class="hljs-string">&#x27;y&#x27;</span>, p=p, observed=purchased)<br /><br />    <span class="hljs-comment"># posterior sampling</span><br />    trace = pm.sample(<span class="hljs-number">1000</span>, tune=<span class="hljs-number">1000</span>)<br /><br /><span class="hljs-comment"># plot posterior distributions</span><br />pm.plot_posterior(trace, var_names=[<span class="hljs-string">&#x27;alpha&#x27;</span>, <span class="hljs-string">&#x27;beta_age&#x27;</span>, <span class="hljs-string">&#x27;beta_income&#x27;</span>])<br />plt.show()</span></pre><figure name="888c" id="888c" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*foICKtvr1zv1VCowphSjrQ.png" data-width="1748" data-height="517" src="https://cdn-images-1.medium.com/max/800/1*foICKtvr1zv1VCowphSjrQ.png"></figure><p name="82d9" id="82d9" class="graf graf--p graf-after--figure">This plot shows the posterior distributions of the intercept (<code class="markup--code markup--p-code">alpha</code>), age coefficient (<code class="markup--code markup--p-code">beta_age</code>), and income coefficient (<code class="markup--code markup--p-code">beta_income</code>). The distributions provide information about the most likely values of the parameters (indicated by the peaks of the distributions), as well as the uncertainty and credibility of the estimates (indicated by the width and shape of the distributions). For example, the plot suggests that age and income are positively associated with the probability of purchasing a product. At the same time, the intercept is close to zero, indicating no strong bias toward purchasing or not purchasing based on the customer&#39;s demographic information.</p><blockquote name="100c" id="100c" class="graf graf--pullquote graf-after--p">Conclusion:</blockquote><p name="fe1b" id="fe1b" class="graf graf--p graf-after--pullquote">Bayesian inference is a powerful approach to machine learning that provides a framework for probabilistic reasoning and decision-making. By incorporating prior knowledge and updating it based on observed data, Bayesian inference can be used to make accurate predictions and decisions in a wide range of applications, from personalized marketing to medical diagnosis.</p><p name="eb63" id="eb63" class="graf graf--p graf-after--p graf--trailing">While Bayesian inference can be more computationally intensive than other machine learning methods, recent algorithms and computing resource advances have made it more accessible and practical for real-world applications. Furthermore, the ability to quantify uncertainty and incorporate prior knowledge makes Bayesian inference particularly valuable in situations where data is limited or noisy.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@the_daft_introvert" class="p-author h-card">Vishal Sharma</a> on <a href="https://medium.com/p/5e98ccdc0d3f"><time class="dt-published" datetime="2023-04-02T09:22:09.050Z">April 2, 2023</time></a>.</p><p><a href="https://medium.com/@the_daft_introvert/utilizing-bayesian-inference-an-approach-to-making-predictions-through-probability-distributions-5e98ccdc0d3f" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 2, 2023.</p></footer></article></body></html>